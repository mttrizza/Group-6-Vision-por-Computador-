# Buenos d√≠as profesor, somos Mattia Rizza y Riccardo Belletti y este es nuestro proyecto final de Visi√≥n por Computador
<img src="img/TraductorRBMR.png" width="500" />

[Link Projecto y Video y Dataset](https://alumnosulpgc-my.sharepoint.com/:f:/g/personal/mattia_rizza101_alu_ulpgc_es/IgC-0DDnFtsgSYzWowwpVGCvAVrgFpJ8NdjnmPI-oWncQes?e=cIqijs)

El objetivo del proyecto ha sido aplicar de manera pr√°ctica los conceptos vistos en clase queriendo crear un verdadero traductor para la **lengua de signos**.  
El proyecto no nace como un trabajo de investigaci√≥n avanzada, sino como un ejercicio completo y realista. Partimos de una idea que podr√≠a parecer banal inicialmente, le hemos a√±adido nuestro *toque personal* y hemos conseguido sacar un buen programa que realmente se podr√≠a usar para ayudar a las personas con esta *discapacidad*.  

Al lanzar el programa se abrir√° el **v√≠deo** y el usuario ver√° que nos encontramos en modo traductor; en este modo el usuario podr√° practicar para **aprender la lengua de signos espa√±ola** y los otros s√≠mbolos que hemos a√±adido para implementar acciones.  
Una vez que el usuario decide que quiere escribir un mensaje, podr√° hacer el **gesto** ü§ü con las manos para entrar en el modo escritura; dentro de este modo el usuario podr√° **componer cualquier mensaje letra por letra** y, una vez terminado, podr√° poner el dedo sobre el icono del **micr√≥fono** para hacer que el ordenador **pronuncie** la frase compuesta.  

Para **acelerar la escritura** hemos a√±adido una funcionalidad que te permite ver en pantalla **palabras recomendadas** mientras est√°s escribiendo, y por lo tanto, si *por ejemplo* est√°s escribiendo "pro" aparecer√°n algunas palabras como "proyecto" y poniendo el dedo encima completar√° la palabra.  

Adem√°s de los gestos para escribir las letras, nosotros hemos a√±adido a nuestro dataset otros **6 gestos** para realizar las siguientes acciones:

- entrar y salir del modo escritura
- borrar toda la frase escrita en modo escritura
- borrar solo la √∫ltima letra escrita
- a√±adir un espacio en la frase
- a√±adir el signo de interrogaci√≥n de apertura
- a√±adir el signo de interrogaci√≥n de cierre
  
A continuaci√≥n es posible ver la leyenda con todos los gestos utilizables. 


<img src="img/Legenda.jpeg" width="400" />

--- 

## Idea general del proyecto

La idea base ha sido mezclar algunos datasets de im√°genes organizados en clases encontrados en **Kaggle** y luego integrarlos con im√°genes hechas por *nosotros*, en aquellas letras que nuestro programa ten√≠a *dificultades* para reconocer correctamente.

En particular:

- hemos recopilado im√°genes en bruto (raw data) que ya estaban organizadas en carpetas
- hemos escrito scripts de Python para automatizar parte del proceso
- hemos usado un notebook de Jupyter para explorar y verificar el dataset

## Estructura del proyecto

La estructura principal del repositorio es la siguiente:
```
Progetto_VC/
‚îÇ
‚îú‚îÄ‚îÄ pycache/
‚îÇ
‚îú‚îÄ‚îÄ inference_classifier.py
‚îÇ
‚îú‚îÄ‚îÄ model.p
‚îÇ
‚îú‚îÄ‚îÄ test_vision.py
‚îú‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ create_dataset.ipynb
‚îú‚îÄ‚îÄ train_classifier.ipynb
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ collect_data.py
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ raw/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ABRIR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_LETRA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_TODO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ CERRAR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ESPACIO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ F/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ H/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MODO_ESCRITURA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ S/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ T/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ U/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ V/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ W/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ X/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Y/
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ new_data/
‚îÇ
‚îÇ
‚îî‚îÄ‚îÄ .DS_Store
```


## Descripci√≥n de las carpetas y de los archivos principales

### utils.py

**Objetivo del m√≥dulo**
El archivo **utils.py** contiene la *l√≥gica matem√°tica de transformaci√≥n de los datos*. Su funci√≥n principal, *get_normalized_landmarks*, act√∫a como un filtro intermedio entre la extracci√≥n en bruto de *+MediaPipe** y la entrada del clasificador.   
El *objetivo* es hacer que los datos sean agn√≥sticos respecto a la posici√≥n y a la distancia de la mano, garantizando que el modelo aprenda la forma del gesto y no su posici√≥n en el espacio.

**Funcionamiento t√©cnico**  
La funci√≥n recibe como entrada el objeto **hand_landmarks** de MediaPipe y aplica una pipeline de transformaci√≥n en tres fases:

**1. Conversi√≥n a coordenadas relativas (invarianza a la traslaci√≥n)**  
Los datos en bruto de MediaPipe son coordenadas absolutas (x, y) normalizadas respecto a las dimensiones de la imagen (0.0 - 1.0). Si us√°ramos estos datos directamente, el modelo aprender√≠a que una mano en la esquina superior izquierda es diferente de una mano en la esquina inferior derecha, aunque hagan el mismo gesto. Para resolver este problema, el c√≥digo establece la mu√±eca (Landmark 0) como origen (0, 0) del sistema cartesiano local. Resta las coordenadas de la mu√±eca a todos los dem√°s puntos:

```python
P'{i} = P{i} - P_{polso}
```
Trova le coordinate del polso (punto 0) per usarle come origine
```python
if index == 0:
    base_x, base_y = landmark_point[0], landmark_point[1]
```

Sottrai la base a tutti i punti (Traslazione dell'origine)
```python
temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x
temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y
```


**2. Flattening (aplanamiento)**  
Los datos se convierten de una lista de parejas bidimensionales [[x1, y1], [x2, y2]...] a un √∫nico vector unidimensional [x1, y1, x2, y2...].

Appiattisci la lista usando itertools
```python
temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
```

**3. Normalizaci√≥n de escala (invarianza a la escala)**
La **mano** puede estar cerca de la c√°mara (coordenadas grandes) o lejos (coordenadas peque√±as). Para hacer que el gesto sea reconocible independientemente de la distancia, los valores se normalizan dividiendo todo por el valor absoluto m√°ximo presente en el vector. Esto fuerza a que todos los datos queden dentro de un rango entre ‚àí 1 y 1.

Normalizza tra -1 e 1
```python
max_value = max(list(map(abs, temp_landmark_list)))

def normalize_(n):
    return n / max_value if max_value != 0 else 0

temp_landmark_list = list(map(normalize_, temp_landmark_list))
```

### create_database.ipynb

**Objetivo del notebook**  
Este script constituye la fase de **Pre-processing** y **Feature Extraction** de la pipeline de Computer Vision. El objetivo no es simplemente leer las im√°genes, sino transformar los datos no estructurados (p√≠xeles de las im√°genes raw) en datos estructurados (coordenadas geom√©tricas de los landmark de la mano), listos para el entrenamiento de un clasificador (por ejemplo Random Forest).

En concreto, el notebook realiza tres tareas cr√≠ticas:

1. **Iteraci√≥n**: Escanea el dataset organizado en directorios.  
2. **Feature Extraction**: Utiliza MediaPipe Hands para detectar el esqueleto de la mano en cada imagen y extraer las coordenadas (x, y) de los 21 puntos clave.  
3. **Serializaci√≥n**: Guarda las listas de features y las etiquetas (labels) en un formato binario comprimido (data.pickle), reduciendo dr√°sticamente el tama√±o de los datos respecto a las im√°genes originales y acelerando el training.

**Requisitos previos y librer√≠as**
Para la ejecuci√≥n correcta, la estructura de directorios debe seguir la taxonom√≠a de clases (por ejemplo data/A, data/B, etc.). Las librer√≠as principales son:

- MediaPipe: para la extracci√≥n de los landmark esquel√©ticos (el ‚Äúcoraz√≥n‚Äù del pre-processing).
- OpenCV (cv2): para la manipulaci√≥n de im√°genes (conversi√≥n BGR -> RGB).
- Pickle: para la serializaci√≥n de objetos Python.
- Matplotlib (opcional): para visualizar las im√°genes durante el debug.

**An√°lisis de la estructura (detalle a nivel de c√≥digo)**  
Celda 1 ‚Äì **Configuraci√≥n del entorno**
Se definen las rutas y se inicializa el modelo est√°tico de MediaPipe. A diferencia del script en tiempo real, aqu√≠ configuramos MediaPipe con static_image_mode=True, optimizado para im√°genes individuales con alta precisi√≥n.

```python
mp_hands = mp.solutions.hands

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)
DATA_DIR = './data'
```

Celda 2 ‚Äì **Extracci√≥n de las features (Core Loop)**  
Esta es la secci√≥n computacionalmente m√°s intensa. El c√≥digo itera sobre cada subcarpeta (que representa una clase/letra) y para cada imagen ejecuta la conversi√≥n.

Pasos t√©cnicos relevantes para cada imagen:

1. **Conversi√≥n de espacio de color**: MediaPipe requiere entrada RGB, mientras que OpenCV carga en BGR.
```python
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
```

2. **Inferencia MediaPipe: se calculan los landmark.**

```python
results = hands.process(img_rgb)
```

3. **Feature Extraction & Normalizaci√≥n** (crucial):
si se detecta una mano, no nos limitamos a extraer coordenadas crudas (x, y respecto a los bordes de la imagen). En su lugar, se invoca la funci√≥n *custom get_normalized_landmarks*

```python
if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            normalized_landmarks = get_normalized_landmarks(hand_landmarks)
            data.append(normalized_landmarks)
            labels.append(dir_)
```

Celda 3 ‚Äì **Serializaci√≥n de los datos**
Los datos procesados se guardan. Este paso crea un ‚Äúcheckpoint‚Äù. Si en el futuro se quiere cambiar el modelo de clasificaci√≥n (por ejemplo pasar de Random Forest a SVM o Red Neuronal), no ser√° necesario reprocesar todas las im√°genes, sino que bastar√° con cargar este archivo pickle.

```python
f = open('data.pickle', 'wb')
pickle.dump({'data': data, 'labels': labels}, f)
f.close()
```

### train_classifier.ipynb

**Objetivo del notebook**  
En este script ocurre la **transici√≥n** desde los datos geom√©tricos (las coordenadas de los landmark extra√≠das en el paso anterior, create_database.ipynb) hasta la creaci√≥n de un modelo de decisi√≥n capaz de clasificar nuevas entradas en tiempo real.

El objetivo es entrenar un algoritmo de Aprendizaje Supervisado para que aprenda a asociar patrones espec√≠ficos de coordenadas (features) con las letras correspondientes (label).

**Librer√≠as utilizadas**  
- *Scikit-learn* (sklearn): librer√≠a est√°ndar de facto para ML en Python. Se utiliza para la gesti√≥n del dataset, creaci√≥n del modelo y c√°lculo de m√©tricas.
- *Pickle & NumPy*: para gesti√≥n eficiente de datos serializados y operaciones matriciales.

**An√°lisis del flujo (detalle t√©cnico)**  
Celdas 1 & 2 ‚Äì **Carga y preparaci√≥n de datos**
El notebook comienza cargando el archivo **dataset.pickle** generado en la fase anterior. Las listas Python se convierten inmediatamente en **NumPy Arrays**, optimizados para c√°lculos vectoriales requeridos por los algoritmos de Scikit-learn, ofreciendo prestaciones superiores respecto a listas est√°ndar.

Celda 3 ‚Äì **Data Splitting y entrenamiento** (el core)
Esta celda ejecuta tres operaciones cr√≠ticas para la validez cient√≠fica del proyecto:

1. **Partitioning** (Train/Test Split): el dataset se divide en dos subconjuntos disjuntos:

**Training Set** *(80%)*: usado por el modelo para aprender las reglas.
**Test Set** *(20%)*: usado para evaluar el rendimiento en datos ‚Äúnunca vistos antes‚Äù.

```python
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)
```

2. **Selecci√≥n del modelo:** se eligi√≥ *Random Forest Classifier*.  
*Motivaci√≥n:* es un m√©todo *‚ÄúEnsemble‚Äù* que construye una multitud de √°rboles de decisi√≥n. Es especialmente adecuado para este proyecto porque gestiona bien datasets con muchas features *(42 coordenadas en total)* y es robusto frente al overfitting (el riesgo de aprender ‚Äúde memoria‚Äù en lugar de generalizar).

3. **Evaluaci√≥n (Accuracy)**: despu√©s del entrenamiento (.fit), el modelo genera predicciones sobre el Test Set. La exactitud (accuracy_score) nos proporciona una m√©trica porcentual fiable sobre la capacidad del modelo para generalizar.

```python
model = RandomForestClassifier()
model.fit(x_train, y_train)
# Haz una prueba con los datos de test para ver qu√© tan bueno es
y_predict = model.predict(x_test)
# Calcula la accuracy
score = accuracy_score(y_predict, y_test)
```
Exactitud del modelo: 99.26%.

Celda 4 ‚Äì **Serializaci√≥n del modelo**
Una vez verificada una exactitud satisfactoria *(t√≠picamente > 95%)*, el **modelo entrenado** se guarda en el archivo **model.p**.  
Este archivo contiene el objeto completo **Random Forest** (con todos sus √°rboles de decisi√≥n y los umbrales matem√°ticos calculados) y ser√° el √∫nico archivo necesario para el script de inferencia en tiempo real (inference_classifier.py).

```python
f = open('model.p', 'wb')
pickle.dump({'model': model}, f)
f.close()
```

### collect_data.py

**Motivaci√≥n y necesidad del script**

Durante las fases preliminares del proyecto, se intent√≥ el entrenamiento utilizando exclusivamente la fusi√≥n de dos datasets p√∫blicos preexistentes. Sin embargo, las pruebas iniciales evidenciaron dos criticidades fundamentales:

1. **Heterogeneidad de los datos**: los datasets originales presentaban condiciones de iluminaci√≥n, fondos y √°ngulos demasiado diferentes respecto al entorno operativo real, llevando a una baja capacidad de generalizaci√≥n del modelo (Domain Shift).

2. **Incompletitud de las clases:** no fue posible encontrar un dataset externo que cubriera perfectamente todas las clases deseadas.

Para *resolver estas problem√°ticas* sin tener que anotar manualmente miles de im√°genes, se desarroll√≥ el script **collect_data.py**. Esta herramienta permite integrar el dataset existente con im√°genes adquiridas directamente en el entorno de uso final, mejorando dr√°sticamente la robustez del modelo.

**Funcionamiento t√©cnico**
El script implementa un sistema de **adquisici√≥n on-demand**. A diferencia de una grabaci√≥n de v√≠deo continua, este enfoque permite al usuario posicionar la mano correctamente y guardar el *frame* solo cuando el gesto es perfecto, garantizando calidad del dato de entrada.  
El funcionamiento se basa en tres bloques l√≥gicos:

1. **Setup de la c√°mara** (alta resoluci√≥n)
Se inicializa la webcam con una resoluci√≥n **HD (1280x720)**.  
Usar una resoluci√≥n m√°s alta en esta fase es crucial para garantizar que MediaPipe (en el siguiente paso) reciba suficientes detalles para extraer los landmark con precisi√≥n.

```python
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
```

2. **Gesti√≥n din√°mica de clases** (File System)
El c√≥digo no requiere pre-crear las carpetas manualmente. Usando la librer√≠a os, el script verifica la entrada del teclado y gestiona autom√°ticamente la estructura de directorios. Si el usuario pulsa la tecla "A", el script comprueba la existencia de la carpeta ./data/raw/A, la crea si es necesario, y calcula el nombre progresivo del archivo para evitar sobrescrituras.

```python
# Convertimos el c√≥digo de la tecla en letra (ej. 97 -> 'a' -> 'A')
lettera = chr(key).upper()
 
# Gesti√≥n autom√°tica de la estructura de las carpetas
folder_path = os.path.join(DATA_DIR, lettera)
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
```

3. **Adquisici√≥n y guardado (I/O)**
En el momento en que se pulsa la tecla, el frame actual se *‚Äúcongela‚Äù* y se guarda en disco mediante **OpenCV**. Esto permite poblar r√°pidamente las clases menos representadas o a√±adir nuevas (como los comandos gestuales personalizados) en pocos segundos.

```python
# Cuenta cu√°ntos archivos ya existen para no sobrescribirlos
count = len(os.listdir(folder_path))
          
# Guarda la imagen
file_name = f"aa{count}.jpg"
cv2.imwrite(os.path.join(folder_path, file_name), frame)
```

### inference_classifier.py

**La infraestructura software y los motores de soporte**

El script **inference_classifier.py** no act√∫a como un simple ejecutor lineal, sino que se configura como un hub de integraci√≥n que orquesta simult√°neamente visi√≥n artificial, interfaces gr√°ficas avanzadas, s√≠ntesis vocal y l√≥gica predictiva.

Para superar los **l√≠mites nativos** de las librer√≠as individuales (como la falta de soporte de transparencia en OpenCV o las operaciones bloqueantes del audio), fue necesario implementar una capa de infraestructura custom antes de entrar en el ciclo principal de procesamiento.

**El motor gr√°fico avanzado** *(Alpha Blending)*

Uno de los retos al desarrollar interfaces modernas con **OpenCV** es la gesti√≥n de la *transparencia*. **OpenCV** gestiona las im√°genes como matrices de p√≠xeles *BGR (Blue-Green-Red)* opacos. Para visualizar iconos modernos *(como el micr√≥fono)* con bordes suaves y fondos transparentes, se implement√≥ la funci√≥n overlay_transparent.

Esta funci√≥n ejecuta una operaci√≥n matem√°tica conocida como **Alpha Blending**. En lugar de sobrescribir brutalmente los p√≠xeles del v√≠deo con los del icono (lo que resultar√≠a en un rect√°ngulo negro alrededor de la imagen), el c√≥digo calcula una media ponderada para cada p√≠xel.

Analizando el c√≥digo, vemos primero la separaci√≥n de canales:

```python
# Separa los canales: BGR (color) y Alpha (transparencia)
overlay_img = overlay_resized[:, :, :3] 
overlay_mask = overlay_resized[:, :, 3:] / 255.0
```

Posteriormente se calcula la m√°scara inversa para el fondo:

```python
background_mask = 1.0 - overlay_mask
```

Finalmente, ocurre la fusi√≥n matricial propiamente dicha:

```python
# Fusiona las im√°genes: (Color del icono * Alpha) + (Fondo * (1 - Alpha))
blended_roi = (overlay_img * overlay_mask + roi * background_mask).astype(np.uint8)
```

Esta √∫nica l√≠nea de c√≥digo vectorial permite obtener una interfaz de usuario fluida.

**Renderizado de texto Unicode** (El puente *OpenCV-Pillow*)  

Otra *limitaci√≥n cr√≠tica* de **OpenCV** es la falta de soporte para **conjuntos de caracteres extendidos** (Unicode). Funciones est√°ndar como *cv2.putText* no son capaces de renderizar caracteres como la **√ë** espa√±ola o el signo de interrogaci√≥n invertido **¬ø**.  
Para resolver el problema, se cre√≥ la funci√≥n **wrapper put_text_utf8**.  
Esta funci√≥n act√∫a como un puente entre dos librer√≠as gr√°ficas distintas:

1. Convierte el frame de v√≠deo del formato OpenCV (array NumPy) al formato Pillow (PIL Image):

```python
img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
```

2. Utiliza el motor de renderizado de Pillow para dibujar el texto usando una fuente TrueType (arial.ttf), que soporta nativamente todos los glifos internacionales.

3. Reconvierten la imagen procesada al formato BGR de OpenCV para poder mostrarla en v√≠deo.

Este enfoque h√≠brido garantiza que la interfaz de usuario sea ling√º√≠sticamente correcta sin sacrificar el rendimiento de la pipeline de v√≠deo.

**Gesti√≥n as√≠ncrona del audio** (*Multithreading*)  

La interacci√≥n **humano-m√°quina** requiere feedback inmediato. Sin embargo, la librer√≠a de s√≠ntesis de voz **pyttsx3** opera en modo bloqueante: cuando se ejecuta el comando **engine.say()**, el procesador espera a que la frase termine antes de pasar a la siguiente instrucci√≥n.  
En un contexto de v√≠deo, esto causar√≠a el *‚Äúcongelamiento‚Äù* de la webcam durante varios segundos cada vez que el ordenador habla.

Para mantener el sistema Real-Time, se introdujo la ejecuci√≥n concurrente mediante el m√≥dulo threading. La funci√≥n run_voice_thread encapsula la l√≥gica de voz en un proceso paralelo:

```python
def run_voice_thread(text):
    t = threading.Thread(target=speak_function, args=(text, VOICE_ID_MANUALE))
    t.start()
```

Al lanzar el hilo con **t.start()**, el sistema operativo crea un nuevo hilo de ejecuci√≥n para la voz.  
El ciclo principal del v√≠deo (while True) contin√∫a por tanto girando a 30 FPS sin interrupciones, mientras que en ‚Äúsegundo plano‚Äù el motor **TTS (Text-to-Speech)** pronuncia la frase.

---------------------------------------------di di parlare anche della funzione speak_function

**El motor NLP (Natural Language Processing)**

Por √∫ltimo, para soportar la funcionalidad de **‚ÄúSugeridor Inteligente‚Äù**, se implement√≥ un motor **NLP** ligero basado en diccionario.  
La elecci√≥n de no utilizar redes neuronales pesadas (como LSTM o Transformers) para esta tarea est√° dictada por la necesidad de mantener baja la latencia.

El diccionario "**DICCIONARIO**" act√∫a como una *Knowledge Base* est√°tica. La funci√≥n get_suggestions_list ejecuta una operaci√≥n de string-matching optimizada sobre la √∫ltima palabra parcial introducida:

```python
def get_suggestions_list(current_sentence):
    if not current_sentence: return []
    parts = current_sentence.split(" ")
    last_fragment = parts[-1]
    if len(last_fragment) == 0: return [] 
    matches = []
    for word in DICCIONARIO:
        if word.startswith(last_fragment) and word != last_fragment:
            matches.append(word)
            if len(matches) >= 3: break 
    return matches
```

Este dise√±o permite obtener sugerencias instant√°neas (complejidad computacional m√≠nima) que se actualizan frame a frame mientras el usuario compone el gesto.

El n√∫cleo operativo del script est√° encapsulado en un **bucle infinito** (while True), que gestiona la sincronizaci√≥n entre la adquisici√≥n del mundo real (Webcam) y el renderizado de la informaci√≥n digital **(GUI)**.

**Adquisici√≥n y normalizaci√≥n del flujo de v√≠deo** 

Al inicio de cada iteraci√≥n, el sistema adquiere el frame bruto de la c√°mara. Sin embargo, antes de cualquier procesamiento, se ejecutan dos operaciones cr√≠ticas de pre-processing:

- **Conversi√≥n de espacio de color**: *MediaPipe*, al estar entrenado sobre datasets RGB, requiere este formato, mientras que OpenCV adquiere nativamente en BGR.
- **Mirroring (Efecto espejo)**: esta operaci√≥n es fundamental para la Usabilidad (UX). Sin el volteo horizontal (flip), mover la mano a la derecha provocar√≠a un movimiento a la izquierda en pantalla, creando confusi√≥n al usuario.




## Diario
En lo que respecta al **diario** de este *proyecto final*, muy a menudo hemos trabajado de manera presencial, ya fuera despu√©s de las clases en la *biblioteca de la universidad* o en otra *biblioteca* cercana a **Las Canteras**.  
Hemos realizado casi todo el proyecto *juntos* y de *forma presencial*, para que ambos pudi√©ramos entender bien lo que hac√≠a el otro y porque ante cualquier **problema** o **duda**, en *persona* se consigue resolver casi de inmediato, en lugar de hacerlo por *tel√©fono*.    
Las pocas veces en las que no consegu√≠amos encontrarnos en persona, utiliz√°bamos **videollamadas por WhatsApp** o, cuando uno pod√≠a y el otro no, trabaj√°bamos de **forma individual** envi√°ndonos mensajes cada vez que se realizaba alguna modificaci√≥n o avance.


## Tecnologie utilizzate

* **Python 3**
* **Jupyter Notebook**
* Librerie standard per la gestione di immagini e file (ad esempio `os`, `opencv`, `numpy`, quando necessario)

Non abbiamo utilizzato framework particolarmente avanzati perch√© l‚Äôobiettivo del progetto era soprattutto capire **il flusso di lavoro**, non ottimizzare le prestazioni.

---

## Metodologia di lavoro
[DA FARE]

## Propuestas de ampliaci√≥n
- Se podr√≠a a√±adir la posibilidad de **traducir gestos din√°micos** y no solo est√°ticos, incorporando tambi√©n la **segunda mano** para la detecci√≥n.
- Se podr√≠a a√±adir la posibilidad de **modificar el idioma** en el que se quiere hablar y, en consecuencia, cambiar autom√°ticamente el **diccionario** seg√∫n el idioma seleccionado.
- **Ampliar el diccionario** con muchas m√°s palabras.




