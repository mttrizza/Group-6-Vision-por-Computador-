# TraductorRBMR

<img src="img/TraductorRBMR.png" width="500" />

Buenos d√≠as profesor, somos **Mattia Rizza** y **Riccardo Belletti** **Grupo 6** y este es nuestro proyecto final de **Visi√≥n por Computador**  

[Link Projecto y Video y Dataset](https://alumnosulpgc-my.sharepoint.com/:f:/g/personal/mattia_rizza101_alu_ulpgc_es/IgC-0DDnFtsgSYzWowwpVGCvAVrgFpJ8NdjnmPI-oWncQes?e=cIqijs)

El objetivo del proyecto ha sido aplicar de manera pr√°ctica los conceptos vistos en clase queriendo crear un verdadero traductor para la **lengua de signos**.  
El proyecto no nace como un trabajo de investigaci√≥n avanzada, sino como un ejercicio completo y realista. Partimos de una idea que podr√≠a parecer banal inicialmente, le hemos a√±adido nuestro *toque personal* y hemos conseguido sacar un buen programa que realmente se podr√≠a usar para ayudar a las personas con esta *discapacidad*.  

Al lanzar el programa se abrir√° el **v√≠deo** y el usuario ver√° que nos encontramos en modo traductor; en este modo el usuario podr√° practicar para **aprender la lengua de signos espa√±ola** y los otros s√≠mbolos que hemos a√±adido para implementar acciones.  
Una vez que el usuario decide que quiere escribir un mensaje, podr√° hacer el **gesto** ü§ü con las manos para entrar en el modo escritura; dentro de este modo el usuario podr√° **componer cualquier mensaje letra por letra** y, una vez terminado, podr√° poner el dedo sobre el icono del **micr√≥fono** para hacer que el ordenador **pronuncie** la frase compuesta.  

Para **acelerar la escritura** hemos a√±adido una funcionalidad que te permite ver en pantalla **palabras recomendadas** mientras est√°s escribiendo, y por lo tanto, si *por ejemplo* est√°s escribiendo "pro" aparecer√°n algunas palabras como "proyecto" y poniendo el dedo encima completar√° la palabra.  

Adem√°s de los gestos para escribir las letras, nosotros hemos a√±adido a nuestro dataset otros **6 gestos** para realizar las siguientes acciones:

- entrar y salir del modo escritura
- borrar toda la frase escrita en modo escritura
- borrar solo la √∫ltima letra escrita
- a√±adir un espacio en la frase
- a√±adir el signo de interrogaci√≥n de apertura
- a√±adir el signo de interrogaci√≥n de cierre
  
A continuaci√≥n es posible ver la leyenda con todos los gestos utilizables. 


<img src="img/Legenda.jpeg" width="400" />

--- 

## Diario y Metodolog√≠a de trabajo
En lo que respecta al **diario** de este *proyecto final*, muy a menudo hemos trabajado de manera presencial, ya fuera despu√©s de las clases en la *biblioteca de la universidad* o en otra *biblioteca* cercana a **Las Canteras** y para finalizar los √∫ltimos detalles, nos reunimos en casa de Riccardo.  
Por la metodolog√≠s de trabajo hemos realizado casi todo el proyecto *juntos* y de *forma presencial*, para que ambos pudi√©ramos entender bien lo que hac√≠a el otro y porque ante cualquier **problema** o **duda**, en *persona* se consigue resolver casi de inmediato, en lugar de hacerlo por *tel√©fono*.    
Las pocas veces en las que no consegu√≠amos encontrarnos en persona, utiliz√°bamos **videollamadas por WhatsApp** o, cuando uno pod√≠a y el otro no, trabaj√°bamos de **forma individual** envi√°ndonos mensajes cada vez que se realizaba alguna modificaci√≥n o avance.

Orden cronol√≥gico del desarrollo del proyecto

## 1. Configuraci√≥n del Entorno de Desarrollo

Para garantizar la reproducibilidad y el aislamiento de las dependencias, el proyecto se desarroll√≥ dentro de un entorno virtual **Anaconda**.

```python
conda create -n proyecto_vc python=3.10
conda activate proyecto_vc
pip install mediapipe==0.10.9
pip install pyttsx3
pip install pillow
```

Se eligi√≥ **Python 3.10** para garantizar la compatibilidad completa con **MediaPipe** y las librer√≠as de soporte utilizadas.

## 2. Dataset: Construcci√≥n y Uni√≥n de las Fuentes

Se utilizaron y unificaron dos *datasets* p√∫blicos descargados de **Kaggle**:

- Spanish Sign Language Alphabet Static
- Lenguaje de Signos Espa√±ol

El objetivo de la uni√≥n fue aumentar la variedad de manos, √°ngulos y condiciones de iluminaci√≥n, mejorando as√≠ la capacidad de generalizaci√≥n del modelo.  

Durante el desarrollo se observ√≥ que algunas letras **(Y, X, W, V, T, H, F)** resultaban poco fiables. Por este motivo:

- se recopilaron manualmente nuevas im√°genes mediante webcam con el script **collect_data.py**
- se **sustituy√≥ progresivamente** parte de las *im√°genes* de los datasets originales por datos recogidos por nosotros, m√°s coherentes con el entorno real de uso

## 3. Pre-procesamiento y Estandarizaci√≥n de los Datos con utils.py

El archivo **utils.py** representa el n√∫cleo matem√°tico del proyecto: act√∫a como ‚Äútraductor‚Äù entre la visi√≥n artificial y el modelo de Machine Learning.

3.1 **Objetivos del Pre-procesamiento**
El pre-procesamiento fue dise√±ado para garantizar:

- **Invariancia a la traslaci√≥n**
El gesto debe ser reconocido independientemente de la posici√≥n de la mano en la imagen.
- **Invariancia de escala**
El gesto debe ser reconocido tanto con la mano cerca como lejos de la c√°mara.
- **Compatibilidad con modelos de Machine Learning**
Los datos deben transformarse en un vector num√©rico adecuado para un clasificador.

3.2 **Pipeline de Procesamiento**

La funci√≥n **pre_process_landmark** aplica los siguientes pasos:

1. **Copia de seguridad**
Se crea una deepcopy de los landmarks para evitar modificar los datos utilizados para el renderizado gr√°fico.

2. **Relativizaci√≥n de las coordenadas**
El *landmark* 0 (mu√±eca) se fija como origen (0,0). Todos los dem√°s *puntos* se expresan como diferencia respecto a la mu√±eca.

3. **Flattening**
La lista de pares (x, y) se transforma en un √∫nico vector unidimensional.

4. **Normalizaci√≥n**
Todos los valores se escalan en el intervalo **[-1, 1]**, mejorando la estabilidad num√©rica y la convergencia del modelo.

**Output final**:  
Un vector de n√∫meros reales listo para ser proporcionado al clasificador.

## 4. Extracci√≥n de Features con create_dataset.ipynb

Este notebook se encarga de transformar las **im√°genes** en **datos num√©ricos**.

Pipeline:
- Carga de las im√°genes organizadas por clase (A, B, C, ‚Ä¶).
- Detecci√≥n de los 21 landmarks de la mano mediante MediaPipe Hands.
- Aplicaci√≥n del pre-procesamiento definido en utils.py.
- Guardado de los datos en formato num√©rico.

**Resultado**: un dataset estructurado y listo para el entrenamiento.

## 5. Entrenamiento del Modelo (train_classifier.ipynb)

5.1 **Elecciones de Proyecto**

Se utiliz√≥ un **Random Forest Classifier** porque:

- es robusto frente al ruido;
- no requiere feature engineering complejo;
- funciona bien con datasets de tama√±o medio-peque√±o.

5.2 **Fases de Entrenamiento**

- Divisi√≥n de los datos:
  - 80% Training Set
  - 20% Test Set

- Entrenamiento del modelo
- Evaluaci√≥n mediante **accuracy score**

Si la precisi√≥n supera el **95%**, el modelo se exporta como archivo est√°tico:  
***model.p***

Este archivo representa el **‚Äúcerebro‚Äù** de la aplicaci√≥n final.

## 6. Aplicaci√≥n en Tiempo Real con inference_classifier.py

Este es el **archivo ejecutable**, el que utiliza el usuario final.

6.1 **Funcionalidades Principales**

- Adquisici√≥n de v√≠deo desde la webcam
- Detecci√≥n de la mano
- Conversi√≥n de los datos visuales a datos matem√°ticos
- Predicci√≥n del signo
- Interfaz gr√°fica aumentada

6.2 **Pipeline L√≥gica**

Fase A ‚Äì **Setup**

- Carga del modelo **model.p**
- Modo fallback si el modelo no est√° presente.

Fase B ‚Äì **Detection**

- **MediaPipe** identifica los *21 landmarks*.
- Dibujo del esqueleto de la mano en pantalla.

Fase C ‚Äì **Puente Visi√≥n ‚Üí AI**

- Conversi√≥n de coordenadas normalizadas a p√≠xeles.
- Pre-procesamiento mediante **utils.py**.

Fase D ‚Äì **Inference**

- Predicci√≥n num√©rica del modelo.
- Traducci√≥n n√∫mero ‚Üí letra mediante un diccionario.

**Output visual**:

- Webcam en tiempo real
- Bounding box de la mano
- Letra reconocida

## 7. Problema Cr√≠tico: Distinci√≥n entre T y F (Profundidad)

Las letras **T y F** resultan casi indistinguibles en **2D**.

7.1 **An√°lisis del Problema**

En una webcam 2D las coordenadas (x, y) son casi id√©nticas dobemos a√±adir im√°genes al dataset provocaba overfitting

7.2 **Soluci√≥n Algor√≠tmica**

Se aprovech√≥ la **coordenada Z** estimada por MediaPipe:

C√°lculo de la diferencia de profundidad entre:

- la punta del √≠ndice
- la punta del pulgar

Regla:

- √≠ndice m√°s cercano a la c√°mara ‚Üí F
- √≠ndice alineado o detr√°s del pulgar ‚Üí T

7.3 **Calibraci√≥n Experimental**

**F**: valores hasta -0.036  
**T**: valores alrededor de -0.024  
**Umbral final**: -0.028

**Resultado**: distinci√≥n estable y reproducible sin necesidad de reentrenar el modelo.

# 8. Modo Escritura y Gesti√≥n de Comandos

Se introdujeron gestos especiales para:

- **Entrar** / **salir** del modo escritura
- Insertar **espacios**
- Borrar **todo**
- Borrar el **√∫ltimo car√°cter**
- Insertar el **signo de interrogaci√≥n**:

Todo esto transforma el reconocedor en un **verdadero sistema de escritura gestual**.

# 9. Text-to-Speech (Accesibilidad)

Para que el sistema sea realmente √∫til a personas con dificultades vocales, se integr√≥ la **s√≠ntesis de voz**.

- **Librer√≠a**: pyttsx3 (offline)
- **Voz**: espa√±ola (b√∫squeda autom√°tica en el sistema)
- **Activaci√≥n**: salida del modo escritura

Cuando el usuario termina la frase, el sistema √©√©lee en voz alta√©√© el texto generado.

## 10. Soporte Unicode (√ë, ¬ø)

**OpenCV** no soporta correctamente caracteres Unicode.
Por ello se integr√≥ **Pillow** para el renderizado del texto:

- Soporte completo para:
  - **√ë**
  - **¬ø**
- Uso de fuentes reales **(Arial)**
- Texto limpio y legible

## 11. Sugeridor Predictivo (NLP Lite)

Se implement√≥ un sistema de **sugerencia l√©xica**:

- Diccionario interno con **~350 palabras** *frecuentes* en espa√±ol
- An√°lisis de la √∫ltima palabra en tiempo real
- Visualizaci√≥n de sugerencias din√°micas

**Interacci√≥n Touchless**
Las sugerencias son seleccionables sin rat√≥n:

- Hover con el √≠ndice
- Barra de carga temporal
- Selecci√≥n autom√°tica

## 12. Interfaz Gr√°fica: Icono del Micr√≥fono

Se a√±adi√≥ feedback visual mediante iconos **PNG** con transparencia:

- mic_blue.png ‚Üí estado idle
- mic_yellow.png ‚Üí hover
- mic_green.png ‚Üí hablado

Si los iconos no est√°n presentes, el sistema utiliza un **fallback gr√°fico**, evitando fallos de ejecuci√≥n.


## Estructura del proyecto

La estructura principal del repositorio es la siguiente:
```
Progetto_VC/
‚îÇ
‚îú‚îÄ‚îÄ pycache/
‚îÇ
‚îú‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ create_dataset.ipynb
‚îú‚îÄ‚îÄ train_classifier.ipynb
‚îÇ
‚îú‚îÄ‚îÄ model.p
‚îÇ
‚îú‚îÄ‚îÄ traductorRBMR.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ collect_data.py
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ raw/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ABRIR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_LETRA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_TODO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ CERRAR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ESPACIO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ F/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ H/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MODO_ESCRITURA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ S/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ T/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ U/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ V/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ W/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ X/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Y/
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ new_data/
‚îÇ
‚îÇ
‚îî‚îÄ‚îÄ .DS_Store
```


## Descripci√≥n de las carpetas y de los archivos principales

### utils.py

**Objetivo del m√≥dulo**
El archivo **utils.py** contiene la *l√≥gica matem√°tica de transformaci√≥n de los datos*. Su funci√≥n principal, *get_normalized_landmarks*, act√∫a como un filtro intermedio entre la extracci√≥n en bruto de *+MediaPipe** y la entrada del clasificador.  
El *objetivo* es hacer que los datos sean agn√≥sticos respecto a la posici√≥n y a la distancia de la **mano**, garantizando que el modelo aprenda la forma del gesto y no su posici√≥n en el espacio.

**Funcionamiento t√©cnico**  
La funci√≥n recibe como entrada el objeto **hand_landmarks** de MediaPipe y aplica una pipeline de transformaci√≥n en tres fases:

**1. Conversi√≥n a coordenadas relativas (invarianza a la traslaci√≥n)**  
Los datos en bruto de MediaPipe son coordenadas absolutas (x, y) normalizadas respecto a las dimensiones de la imagen (0.0 - 1.0).   
Si us√°ramos estos **datos** directamente, el modelo aprender√≠a que una mano en la **esquina superior izquierda** es diferente de una mano en la **esquina inferior derecha**, aunque hagan el mismo gesto.  
Para resolver este problema, el c√≥digo establece la mu√±eca (*Landmark 0*) como origen (*0, 0*) del sistema cartesiano local.  
Resta las coordenadas de la mu√±eca a todos los dem√°s puntos:

```python
P'{i} = P{i} - P_{polso}
```
Encuentre las coordenadas de la mu√±eca (punto 0) para utilizarlas como origen.
```python
if index == 0:
    base_x, base_y = landmark_point[0], landmark_point[1]
```
Resta la base a todos los puntos (traslaci√≥n del origen)
```python
temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x
temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y
```


**2. Flattening (aplanamiento)**  
Los datos se convierten de una lista de parejas bidimensionales [**[x1, y1], [x2, y2]...**] a un √∫nico vector unidimensional [**x1, y1, x2, y2...**].

Aplana la lista utilizando itertools.
```python
temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
```

**3. Normalizaci√≥n de escala (invarianza a la escala)**
La **mano** puede estar cerca de la c√°mara (coordenadas grandes) o lejos (coordenadas peque√±as).  
Para hacer que el gesto sea reconocible independientemente de la distancia, los valores se normalizan dividiendo todo por el **valor absoluto m√°ximo** presente en el vector.  
Esto fuerza a que todos los datos queden dentro de un rango entre **‚àí 1** y **1**.

Normalizza tra **-1 y 1**
```python
max_value = max(list(map(abs, temp_landmark_list)))

def normalize_(n):
    return n / max_value if max_value != 0 else 0

temp_landmark_list = list(map(normalize_, temp_landmark_list))
```

### create_database.ipynb

**Objetivo del notebook**  
Este script constituye la fase de **Pre-processing** y **Feature Extraction** de la pipeline de Computer Vision. El objetivo no es simplemente leer las im√°genes, sino transformar los datos no estructurados (p√≠xeles de las im√°genes raw) en datos estructurados (coordenadas geom√©tricas de los landmark de la mano), listos para el entrenamiento de un clasificador (por ejemplo Random Forest).

En concreto, el notebook realiza tres tareas cr√≠ticas:

1. **Iteraci√≥n**: Escanea el dataset organizado en directorios.  
2. **Feature Extraction**: Utiliza MediaPipe Hands para detectar el esqueleto de la mano en cada imagen y extraer las coordenadas (x, y) de los 21 puntos clave.  
3. **Serializaci√≥n**: Guarda las listas de features y las etiquetas (labels) en un formato binario comprimido (data.pickle), reduciendo dr√°sticamente el tama√±o de los datos respecto a las im√°genes originales y acelerando el training.

**Requisitos previos y librer√≠as**
Para la ejecuci√≥n correcta, la estructura de directorios debe seguir la taxonom√≠a de clases (por ejemplo data/A, data/B, etc.). Las librer√≠as principales son:

- **MediaPipe**: para la extracci√≥n de los landmark esquel√©ticos (el ‚Äúcoraz√≥n‚Äù del pre-processing).
- **OpenCV** (*cv2*): para la manipulaci√≥n de im√°genes (conversi√≥n BGR -> RGB).
- **Pickle**: para la serializaci√≥n de objetos Python.

**An√°lisis de la estructura (detalle a nivel de c√≥digo)**  
Celda 1 ‚Äì **Configuraci√≥n del entorno**
Se definen las rutas y se inicializa el modelo est√°tico de MediaPipe. A diferencia del script en tiempo real, aqu√≠ configuramos MediaPipe con static_image_mode=True, optimizado para im√°genes individuales con alta precisi√≥n.

```python
mp_hands = mp.solutions.hands

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)
DATA_DIR = './data'
```

Celda 2 ‚Äì **Extracci√≥n de las features (Core Loop)**  
Esta es la secci√≥n computacionalmente m√°s intensa. El c√≥digo itera sobre cada subcarpeta (que representa una clase/letra) y para cada imagen ejecuta la conversi√≥n.

Pasos t√©cnicos relevantes para cada imagen:

1. **Conversi√≥n de espacio de color**: MediaPipe requiere entrada RGB, mientras que OpenCV carga en BGR.
```python
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
```

2. **Inferencia MediaPipe: se calculan los landmark.**

```python
results = hands.process(img_rgb)
```

3. **Feature Extraction & Normalizaci√≥n** (crucial):
si se detecta una mano, no nos limitamos a extraer coordenadas crudas (x, y respecto a los bordes de la imagen). En su lugar, se invoca la funci√≥n *custom get_normalized_landmarks*

```python
if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            normalized_landmarks = get_normalized_landmarks(hand_landmarks)
            data.append(normalized_landmarks)
            labels.append(dir_)
```

Celda 3 ‚Äì **Serializaci√≥n de los datos**
Los datos procesados se guardan. Este paso crea un ‚Äúcheckpoint‚Äù. Si en el futuro se quiere cambiar el modelo de clasificaci√≥n (por ejemplo pasar de Random Forest a SVM o Red Neuronal), no ser√° necesario reprocesar todas las im√°genes, sino que bastar√° con cargar este archivo pickle.

```python
f = open('data.pickle', 'wb')
pickle.dump({'data': data, 'labels': labels}, f)
f.close()
```

### train_classifier.ipynb

**Objetivo del notebook**  
En este script ocurre la **transici√≥n** desde los datos geom√©tricos (las coordenadas de los landmark extra√≠das en el paso anterior, create_database.ipynb) hasta la creaci√≥n de un modelo de decisi√≥n capaz de clasificar nuevas entradas en tiempo real.

El objetivo es entrenar un algoritmo de Aprendizaje Supervisado para que aprenda a asociar patrones espec√≠ficos de coordenadas (features) con las letras correspondientes (label).

**Librer√≠as utilizadas**  
- *Scikit-learn* (sklearn): librer√≠a est√°ndar de facto para ML en Python. Se utiliza para la gesti√≥n del dataset, creaci√≥n del modelo y c√°lculo de m√©tricas.
- *Pickle & NumPy*: para gesti√≥n eficiente de datos serializados y operaciones matriciales.

**An√°lisis del flujo (detalle t√©cnico)**  
Celdas 1 & 2 ‚Äì **Carga y preparaci√≥n de datos**
El notebook comienza cargando el archivo **dataset.pickle** generado en la fase anterior. Las listas Python se convierten inmediatamente en **NumPy Arrays**, optimizados para c√°lculos vectoriales requeridos por los algoritmos de Scikit-learn, ofreciendo prestaciones superiores respecto a listas est√°ndar.

Celda 3 ‚Äì **Data Splitting y entrenamiento** (el core)
Esta celda ejecuta tres operaciones cr√≠ticas para la validez cient√≠fica del proyecto:

1. **Partitioning** (Train/Test Split): el dataset se divide en dos subconjuntos disjuntos:

**Training Set** *(80%)*: usado por el modelo para aprender las reglas.
**Test Set** *(20%)*: usado para evaluar el rendimiento en datos ‚Äúnunca vistos antes‚Äù.

```python
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)
```

2. **Selecci√≥n del modelo:** se eligi√≥ *Random Forest Classifier*.  
*Motivaci√≥n:* es un m√©todo *‚ÄúEnsemble‚Äù* que construye una multitud de √°rboles de decisi√≥n. Es especialmente adecuado para este proyecto porque gestiona bien datasets con muchas features *(42 coordenadas en total)* y es robusto frente al overfitting (el riesgo de aprender ‚Äúde memoria‚Äù en lugar de generalizar).

3. **Evaluaci√≥n (Accuracy)**: despu√©s del entrenamiento (.fit), el modelo genera predicciones sobre el Test Set. La exactitud (accuracy_score) nos proporciona una m√©trica porcentual fiable sobre la capacidad del modelo para generalizar.

```python
model = RandomForestClassifier()
model.fit(x_train, y_train)
# Haz una prueba con los datos de test para ver qu√© tan bueno es
y_predict = model.predict(x_test)
# Calcula la accuracy
score = accuracy_score(y_predict, y_test)
```
Exactitud del modelo: 99.26%.

Celda 4 ‚Äì **Serializaci√≥n del modelo**
Una vez verificada una exactitud satisfactoria *(t√≠picamente > 95%)*, el **modelo entrenado** se guarda en el archivo **model.p**.  
Este archivo contiene el objeto completo **Random Forest** (con todos sus √°rboles de decisi√≥n y los umbrales matem√°ticos calculados) y ser√° el √∫nico archivo necesario para el script de inferencia en tiempo real (inference_classifier.py).

```python
f = open('model.p', 'wb')
pickle.dump({'model': model}, f)
f.close()
```

### collect_data.py

**Motivaci√≥n y necesidad del script**

Durante las fases preliminares del proyecto, se intent√≥ el entrenamiento utilizando exclusivamente la fusi√≥n de dos datasets p√∫blicos preexistentes. Sin embargo, las pruebas iniciales evidenciaron dos criticidades fundamentales:

1. **Heterogeneidad de los datos**: los datasets originales presentaban condiciones de iluminaci√≥n, fondos y √°ngulos demasiado diferentes respecto al entorno operativo real, llevando a una baja capacidad de generalizaci√≥n del modelo (Domain Shift).

2. **Incompletitud de las clases:** no fue posible encontrar un dataset externo que cubriera perfectamente todas las clases deseadas.

Para *resolver estas problem√°ticas* sin tener que anotar manualmente miles de im√°genes, se desarroll√≥ el script **collect_data.py**. Esta herramienta permite integrar el dataset existente con im√°genes adquiridas directamente en el entorno de uso final, mejorando dr√°sticamente la robustez del modelo.

**Funcionamiento t√©cnico**
El script implementa un sistema de **adquisici√≥n on-demand**. A diferencia de una grabaci√≥n de v√≠deo continua, este enfoque permite al usuario posicionar la mano correctamente y guardar el *frame* solo cuando el gesto es perfecto, garantizando calidad del dato de entrada.  
El funcionamiento se basa en tres bloques l√≥gicos:

1. **Setup de la c√°mara** (alta resoluci√≥n)
Se inicializa la webcam con una resoluci√≥n **HD (1280x720)**.  
Usar una resoluci√≥n m√°s alta en esta fase es crucial para garantizar que MediaPipe (en el siguiente paso) reciba suficientes detalles para extraer los landmark con precisi√≥n.

```python
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
```

2. **Gesti√≥n din√°mica de clases** (File System)
El c√≥digo no requiere pre-crear las carpetas manualmente. Usando la librer√≠a os, el script verifica la entrada del teclado y gestiona autom√°ticamente la estructura de directorios. Si el usuario pulsa la tecla "A", el script comprueba la existencia de la carpeta ./data/raw/A, la crea si es necesario, y calcula el nombre progresivo del archivo para evitar sobrescrituras.

```python
# Convertimos el c√≥digo de la tecla en letra (ej. 97 -> 'a' -> 'A')
lettera = chr(key).upper()
 
# Gesti√≥n autom√°tica de la estructura de las carpetas
folder_path = os.path.join(DATA_DIR, lettera)
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
```

3. **Adquisici√≥n y guardado (I/O)**
En el momento en que se pulsa la tecla, el frame actual se *‚Äúcongela‚Äù* y se guarda en disco mediante **OpenCV**. Esto permite poblar r√°pidamente las clases menos representadas o a√±adir nuevas (como los comandos gestuales personalizados) en pocos segundos.

```python
# Cuenta cu√°ntos archivos ya existen para no sobrescribirlos
count = len(os.listdir(folder_path))
          
# Guarda la imagen
file_name = f"aa{count}.jpg"
cv2.imwrite(os.path.join(folder_path, file_name), frame)
```

### traductorRBMR.py

**La infraestructura software y los motores de soporte**

El script **inference_classifier.py** no act√∫a como un simple ejecutor lineal, sino que se configura como un hub de integraci√≥n que orquesta simult√°neamente visi√≥n artificial, interfaces gr√°ficas avanzadas, s√≠ntesis vocal y l√≥gica predictiva.

Para superar los **l√≠mites nativos** de las librer√≠as individuales (como la falta de soporte de transparencia en OpenCV o las operaciones bloqueantes del audio), fue necesario implementar una capa de infraestructura custom antes de entrar en el ciclo principal de procesamiento.

**El motor gr√°fico avanzado** *(Alpha Blending)*

Uno de los retos al desarrollar interfaces modernas con **OpenCV** es la gesti√≥n de la *transparencia*. **OpenCV** gestiona las im√°genes como matrices de p√≠xeles *BGR (Blue-Green-Red)* opacos. Para visualizar iconos modernos *(como el micr√≥fono)* con bordes suaves y fondos transparentes, se implement√≥ la funci√≥n overlay_transparent.

Esta funci√≥n ejecuta una operaci√≥n matem√°tica conocida como Alpha Blending. En lugar de sobrescribir directamente los p√≠xeles del v√≠deo con los del icono (lo que resultar√≠a en un antiest√©tico rect√°ngulo negro alrededor de la imagen), el c√≥digo calcula una media ponderada para cada p√≠xel bas√°ndose en su nivel de transparencia.

Analizando el c√≥digo, vemos primero la separaci√≥n y normalizaci√≥n de los canales:

```python
# Separa los canales: BGR (color) y Alpha (transparencia)
overlay_img = overlay_resized[:, :, :3] 
overlay_mask = overlay_resized[:, :, 3:] / 255.0
```

Posteriormente, se calcula la m√°scara inversa para el fondo (donde el icono es transparente, el fondo debe verse):
```python
background_mask = 1.0 - overlay_mask
```

Finalmente, ocurre la fusi√≥n matricial mediante √°lgebra lineal con NumPy:
```python
# Fusiona las im√°genes: (Color del icono * Alpha) + (Fondo * (1 - Alpha))
blended_roi = (overlay_img * overlay_mask + roi * background_mask).astype(np.uint8)
```

Esta √∫nica l√≠nea de c√≥digo vectorial permite obtener una interfaz de usuario fluida.

**Renderizado de texto Unicode** (El puente *OpenCV-Pillow*)  

Otra *limitaci√≥n cr√≠tica* de **OpenCV** es la falta de soporte para **conjuntos de caracteres extendidos** (Unicode). Funciones est√°ndar como *cv2.putText* no son capaces de renderizar caracteres como la **√ë** espa√±ola o el signo de interrogaci√≥n invertido **¬ø**.  
Para resolver el problema, se cre√≥ la funci√≥n **wrapper put_text_utf8**.  
Esta funci√≥n act√∫a como un puente entre dos librer√≠as gr√°ficas distintas:

1. Convierte el **frame de v√≠deo** del formato **OpenCV** (*array NumPy**) al formato **Pillow** (*PIL Image*):

```python
img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
```

2. Utiliza el motor de renderizado de **Pillow** para dibujar el texto usando una fuente **TrueType** (*arial.ttf*), que soporta nativamente todos los glifos internacionales.

3. Reconvierten la **imagen procesada** al formato **BGR** de **OpenCV** para poder mostrarla en *v√≠deo*.

Este **enfoque h√≠brido** garantiza que la interfaz de usuario sea ling√º√≠sticamente correcta sin sacrificar el rendimiento de la pipeline de v√≠deo.
Adem√°s de la **conversi√≥n de formatos**, la funci√≥n implementa dos mecanismos de seguridad importantes:

1.  **Fallback de Tipograf√≠a** *(Portabilidad)*: El sistema intenta cargar la fuente vectorial *arial.ttf* para asegurar una **est√©tica moderna**. Sin embargo, dado que las fuentes disponibles var√≠an seg√∫n el sistema operativo, se encapsul√≥ la carga en un **bloque de manejo de errores**.  
Si el archivo no se encuentra, el sistema carga autom√°ticamente una fuente predeterminada en lugar de detener la ejecuci√≥n:

```python
try:
    font = ImageFont.truetype("arial.ttf", font_size)
except IOError:
    # Mecanismo de seguridad si falta la fuente
    font = ImageFont.load_default()
```

2.  **Correcci√≥n del Espacio de Color**: Existe una discrepancia entre c√≥mo las librer√≠as interpretan los colores: **OpenCV** utiliza el **est√°ndar BGR (Blue-Green-Red)**, mientras que **Pillow** utiliza **RGB**. Si pas√°ramos el color directamente, el texto rojo aparecer√≠a azul y viceversa.  
Por ello, el c√≥digo realiza una inversi√≥n manual de los canales de color antes de dibujar:
```python
# Inversi√≥n de canales: de BGR (OpenCV) a RGB (Pillow)
color_rgb = (color[2], color[1], color[0])
```

**Gesti√≥n As√≠ncrona del Audio y Arquitectura Multihilo** (*Multithreading*)  

Uno de los **desaf√≠os cr√≠ticos** en los sistemas interactivos en tiempo real es la gesti√≥n de la **latencia**. La operaci√≥n m√°s costosa en t√©rminos de tiempo de ejecuci√≥n no es el reconocimiento de imagen, sino la *s√≠ntesis vocal*.

La librer√≠a **pyttsx3** opera nativamente en modo bloqueante: la funci√≥n **engine.runAndWait()** detiene la ejecuci√≥n del procesador hasta que la frase completa ha sido pronunciada.  
Si el ordenador debe decir *"Hola, ¬øc√≥mo est√°s?"*, el proceso tarda entre **2 y 3 segundos**.  
En una arquitectura de un solo hilo (*Single-Threaded*), esto implicar√≠a congelar el flujo de v√≠deo de la webcam durante ese tiempo, destruyendo la experiencia de usuario.

Para resolver este cuello de botella y mantener el sistema fluido a **30 FPS**, se implement√≥ una arquitectura **Multihilo (Multithreading)** que desacopla el bucle de renderizado (**V√≠deo**) del bucle de procesamiento (**Audio**).

1. **Orquestaci√≥n de Hilos (run_voice_thread)**
La funci√≥n **run_voice_thread** act√∫a como el punto de entrada para la ejecuci√≥n concurrente.  
En lugar de ejecutar el audio directamente, instancia un Worker Thread:

```python
def run_voice_thread(text):
    t = threading.Thread(target=speak_function, args=(text, VOICE_ID_MANUALE))
    t.start()
```

*Desacoplamiento*: Al invocar **t.start()**, el sistema operativo crea un nuevo flujo de ejecuci√≥n paralelo.

**Resultado**: El **Main Thread** (encargado del v√≠deo y la IA) queda libre inmediatamente para procesar el siguiente *frame*, mientras que el audio se procesa en segundo plano.

2. **L√≥gica de Configuraci√≥n Din√°mica (speak_function)**
La funci√≥n **speak_function**, que se ejecuta dentro del hilo secundario, no se limita a reproducir sonido. Implementa una *l√≥gica robusta* de autconfiguraci√≥n y localizaci√≥n para garantizar que el sistema funcione correctamente en diferentes ordenadores.

Analizando el c√≥digo, vemos tres pasos clave:

A. **Selecci√≥n Autom√°tica del Idioma**: Dado que el proyecto est√° dise√±ado para la Lengua de Signos Espa√±ola, el sistema no asume una configuraci√≥n predeterminada.  
En su lugar, itera sobre los drivers de **voz** instalados en el sistema operativo buscando expl√≠citamente una **voz hispana**: 

```python
voices = engine.getProperty('voices')
for v in voices:
    # B√∫squeda heur√≠stica de drivers en espa√±ol
    if "spanish" in v.name.lower() or "esp" in v.name.lower():
        engine.setProperty('voice', v.id)
        break
```

Este **algoritmo** de b√∫squeda garantiza la portabilidad del software: funcionar√° tanto en un Windows configurado en ingl√©s como en uno en espa√±ol, siempre que exista un paquete de voz compatible.

B. **Configuraci√≥n de Velocidad**: Se ajusta la velocidad de habla (rate) a **140 palabras** por minuto para asegurar una dicci√≥n clara y natural, adecuada para fines educativos o de asistencia.

```python
engine.setProperty('rate', 140)
```

C. **Tolerancia a Fallos**: Toda la l√≥gica de audio est√° encapsulada en un bloque try...except.
```python
except Exception as e: print(f"Errore Audio: {e}")
```

**El motor NLP (Natural Language Processing)**
Por √∫ltimo, para elevar la experiencia de usuario con la funcionalidad de ‚Äú**Sugeridor Inteligente**‚Äù, se implement√≥ un motor de **NLP ligero y determinista**.

La decisi√≥n t√©cnica de no utilizar **redes neuronales profundas** (como LSTM, Transformers o BERT) para esta tarea espec√≠fica fue dictada por la necesidad de priorizar la baja latencia.   
En un sistema de visi√≥n artificial que ya consume recursos de **GPU/CPU** para procesar *30 im√°genes por segundo*, a√±adir un modelo de lenguaje pesado habr√≠a comprometido la fluidez del v√≠deo.

**Estructura y Algoritmo:** El sistema se apoya en una **Knowledge Bas**e est√°tica (la lista **DICCIONARIO**), que ha sido curada manualmente para incluir:
- Palabras de uso com√∫n (*HOLA*, *GRACIAS*, *POR FAVOR*).
- Vocabulario espec√≠fico del contexto acad√©mico/universitario (*PROYECTO*, *PROFESOR*, *EXAMEN*, *VISI√ìN*).

La funci√≥n **get_suggestions_list** implementa un algoritmo de **B√∫squeda de Prefijos** (Prefix Matching).   
Analiza la frase en construcci√≥n en tiempo real y a√≠sla el √∫ltimo fragmento escrito para ofrecer candidatos compatibles.

```python
def get_suggestions_list(current_sentence):
    if not current_sentence: return []
    parts = current_sentence.split(" ")
    last_fragment = parts[-1] # A√≠sla el sufijo actual (ej. "PR")
    if len(last_fragment) == 0: return [] 
    matches = []
    for word in DICCIONARIO:
        # Busca palabras que empiecen por el fragmento (startswith)
        # y evita sugerir la palabra si ya est√° completa
        if word.startswith(last_fragment) and word != last_fragment:
            matches.append(word)
            # Optimizaci√≥n: Early Exit al encontrar 3 candidatos para no saturar la UI
            if len(matches) >= 3: break 
    return matches
```

Este dise√±o permite obtener sugerencias instant√°neas que se actualizan frame a frame mientras el usuario compone el gesto.

**Arquitectura del Ciclo de Ejecuci√≥n (Runtime Loop)**
Una vez inicializados los subsistemas de soporte (Gr√°ficos, Audio, NLP), el control del programa pasa al n√∫cleo operativo.

El *script* est√° encapsulado en un bucle infinito (while True), que act√∫a como orquestador central gestionando la sincronizaci√≥n estricta entre la adquisici√≥n del mundo real (**Webcm**) y el renderizado de la informaci√≥n digital (**GUI**).

**Adquisici√≥n y normalizaci√≥n del flujo de v√≠deo** 
Al inicio de cada iteraci√≥n, el sistema adquiere el frame bruto de la c√°mara

```python
while True:
    ret, frame = cap.read()
    if not ret: break
```

Sin embargo, antes de pasar a la fase de inferencia o dibujo, se ejecutan dos operaciones cr√≠ticas de **pre-processing** para adecuar los datos:

- **Conversi√≥n de espacio de color**: **MediaPipe**, al estar entrenado sobre datasets *RGB*, requiere este formato espec√≠fico, mientras   que OpenCV adquiere nativamente en BGR. La conversi√≥n es necesaria para garantizar la precisi√≥n del modelo.

- **Mirroring** (*Efecto espejo*): Esta operaci√≥n es fundamental para la Usabilidad (*UX*). Sin el volteo horizontal (*flip*), mover la   mano f√≠sica hacia la derecha provocar√≠a un movimiento hacia la izquierda en la pantalla (como una c√°mara de vigilancia), creando una disonancia cognitiva que har√≠a imposible interactuar con los botones.

**Renderizado de la Interfaz Din√°mica (GUI)**
La interfaz de usuario no es est√°tica, sino contextual: cambia seg√∫n el estado del sistema. El c√≥digo utiliza una l√≥gica condicional para decidir qu√© elementos dibujar en pantalla.

**Modo Escritura** vs. **Modo Traductor El booleano** *is_writing_mode* act√∫a como un guardian gr√°fico:

- Si es False (**Traductor**): La interfaz es minimalista (una barra gris), invitando al usuario a realizar el gesto de activaci√≥n ("**ROCK**").

- Si es True (Escritura): Se renderiza el "**Dashboard**" completo:
  - La **Barra Verde** superior, que contiene la frase en construcci√≥n.
  - El **Bot√≥n de Micr√≥fono**, un objeto con estados (Azul = Reposo, Amarillo = Hover, Verde = Activo).
  - Las **Cajas de Sugerencias**, generadas din√°micamente iterando sobre la lista current_suggestions.

L√≥gica de los **Botones Virtuales** (*Interacci√≥n Sin Contacto*) Uno de los aspectos m√°s innovadores es la implementaci√≥n de botones clicables sin contacto f√≠sico.  
Dado que **no existe** un rat√≥n o pantalla t√°ctil, el sistema debe simular el "*clic*" usando solo la posici√≥n de la mano mediante un algoritmo en tres fases: **Mapping**, **Collision Detection** y **Temporal Filtering**.

1. **Mapeo de Coordenadas (Mapping)**:

**MediaPipe** devuelve coordenadas normalizadas (0.0‚Üí1.0). Para interactuar con la GUI, estas deben proyectarse en el espacio de p√≠xeles de la pantalla (**1280√ó720**):

```python
index_x = int((1 - hand_landmarks.landmark[8].x) * W)
index_y = int(hand_landmarks.landmark[8].y * H)
```

2. **Detecci√≥n de Colisiones (Collision Detection)**:

El sistema verifica si el punto **(x,y)** del dedo √≠ndice cae dentro del rect√°ngulo de un bot√≥n (*Bounding Box*).

```python
if BTN_PARLA_X < index_x < (BTN_PARLA_X + BTN_PARLA_W) and 
   BTN_PARLA_Y < index_y < (BTN_PARLA_Y + BTN_PARLA_H):
       is_hovering_any_ui = True
```

3. **Filtrado Temporal (Dwell Time):**

El problema principal de las interfaces gestuales es el efecto "**Midas Touch**": el riesgo de clicar todo lo que se toca accidentalmente. Para evitar **falsos positivos**, se implement√≥ un mecanismo de **Dwell Time** (tiempo de permanencia).   
El usuario debe mantener el dedo sobre el bot√≥n por un tiempo prefijado (ej. 1.0 segundo) para confirmar la intenci√≥n.

Se proporciona un **Feedback Visual Progresivo**: una barra de carga amarilla se dibuja proporcionalmente al tiempo transcurrido:

```python
elapsed = time.time() - hover_start_time
load_w = int((elapsed / 1.0) * BTN_W)
cv2.rectangle(frame, ..., (BTN_X + load_w, ...), (0, 255, 255), -1)
```

Solo cuando **elapsed >= 1.0**, se dispara el evento

4. **Gesti√≥n Din√°mica de Sugerencias**

Los **botones de sugerencias** se adaptan: 
- si no hay sugerencias, desaparecen;
- si hay 3, aparecen alineados calculando sus coordenadas en tiempo real:

```python
for i, word in enumerate(current_suggestions):
    bx = SUGG_START_X + (SUGG_W + SUGG_GAP) * i
    # Renderizado del rect√°ngulo y texto...
```

Una vez que el sistema **detecta** que el usuario no est√° interactuando con los botones (es decir, is_hovering_any_ui == False), entra en juego el proceso de reconocimiento de gestos.

Esta fase no se limita a preguntar ¬´*¬øqu√© letra es?*¬ª, sino que aplica una serie de filtros l√≥gicos y temporales para corregir los errores t√≠picos de la visi√≥n artificial.  
El **primer paso** es consultar el modelo **Random Forest**. En lugar de preguntar simplemente por la clase ganadora (*model.predict*), el c√≥digo solicita las probabilidades (*model.predict_proba*).

```python
features = get_normalized_landmarks(hand_landmarks)
prediction_proba = model.predict_proba([np.asarray(features)])
max_prob = np.max(prediction_proba)
```

Esto permite **implementar un filtro de confianza**:

```python
if max_prob < MIN_CONFIDENCE:
    # Ignora el gesto si el modelo no es lo suficientemente seguro.
```

Esto **evita** que el sistema escriba caracteres aleatorios cuando la mano est√° en transici√≥n o en una posici√≥n ambigua, lo que reduce dr√°sticamente el ¬´ruido¬ª de fondo.

### Correcci√≥n de Errores y Post-Procesamiento

Los modelos basados √∫nicamente en im√°genes **2D** suelen confundir gestos similares. Para resolver este problema, se han inyectado en el c√≥digo correctores l√≥gicos basados en la geometr√≠a **3D** y en el an√°lisis temporal.

A. **Correcci√≥n Geom√©trica 3D** (*Distinci√≥n T vs F*) 
Las letras **'T' y 'F'** en el lenguaje de se√±as son muy similares visualmente, pero difieren en la profundidad (qu√© dedo est√° delante del otro). **MediaPipe** proporciona la **coordenada Z** (*profundidad relativa*).   
El c√≥digo calcula la distancia relativa en el eje Z entre la punta del pulgar y la del √≠ndice:

```python
diff_z = index_tip_z - thumb_tip_z
# Si la diferencia de profundidad cruza el umbral...
if diff_z < UMBRAL_LIMIT:
    predicted_character = 'F'
else:
    predicted_character = 'T'
```

B. **An√°lisis Temporal Din√°mico (Distinci√≥n N vs √ë)**
La **'N' y la '√ë'** tienen la misma forma de mano, pero la **'√ë'** implica un movimiento ondulatorio lateral.  
Un clasificador est√°tico no puede percibir el movimiento. Para solucionar esto, el sistema mantiene una memoria hist√≥rica (x_history) de las √∫ltimas **20 posiciones** del la mu√±eca:

```python
x_history.append(wrist_x)
if len(x_history) > 20:
    x_history.pop(0)

# Calcula la amplitud del movimiento reciente
movement = max(x_history) - min(x_history)
if predicted_character == 'N' and movement > UMBRAL_MOVIMIENTO_N:
    predicted_character = '√ë'
```

Si el sistema detecta la forma "**N**" PERO existe una oscilaci√≥n significativa, "asciende" la predicci√≥n a "**√ë**", convirtiendo un modelo est√°tico en uno capaz de entender din√°micas temporales.

### Estabilizaci√≥n Temporal (Anti-Flickering)
Una vez determinada la letra (ej. "A"), no podemos escribirla inmediatamente. Los modelos de **Machine Learning** tienden a "parpadear" o fluctuar (ej. A-A-B-A-A) cientos de veces por segundo.  
Para evitar escribir "AAAAA" involuntariamente, se ha implementado un **Temporizador de Confirmaci√≥n** (*CONFIRMATION_TIME = 1.5 segundos*). El sistema verifica la estabilidad de la predicci√≥n:

```python
is_stable = (predicted_character == last_char_detected)
```

- Si la letra cambia: El temporizador se **reinicia**
- Si la letra permanece igual: El temporizador **avanza**

Durante la espera, el usuario recibe un **feedback visual inmediato**: un **c√≠rculo** de carga dibujado alrededor de la mano (*cv2.ellipse*), que se rellena progresivamente.  
Esto comunica al usuario: "*He entendido que quieres hacer la A, mantenla quieta un momento m√°s..*.".

Para **lograr** este efecto, el c√≥digo calcula el √°ngulo del arco bas√°ndose en el tiempo transcurrido y lo dibuja sobre el frame en tiempo real:

```python
# C√°lculo del porcentaje de completado (0.0 a 1.0)
progress = (time.time() - start_time) / CONFIRMATION_TIME
# Convertir a grados (0 a 360) para el arco
angle = int(progress * 360)
# Dibujar el arco progresivo alrededor de la mu√±eca
# -90 indica que el dibujo comienza desde la parte superior (las 12 en punto)
cv2.ellipse(frame, (wrist_x, wrist_y), (60, 60), -90, 0, angle, (0, 255, 0), 2)
```

### La M√°quina de Estados (Ejecuci√≥n de Comandos)

Cuando el temporizador expira (*elapsed >= CONFIRMATION_TIME*), el sistema ejecuta la acci√≥n asociada al gesto reconocido. Aqu√≠ el c√≥digo act√∫a como una m√°quina de **estados finitos**.

Estado 1: **Cambio de Modalidad (SWITCH)**
Si el gesto es "**MODO_ESCRITURA**" (Rock), invierte el estado booleano *is_writing_mode*.

```python
is_writing_mode = not is_writing_mode
```

Estado 2: **Edici√≥n de Texto** 
Si estamos en modo escritura, el gesto se traduce en la manipulaci√≥n de la cadena sentence:
- **Caracteres est√°ndar**: Se concatenan a la cadena.
- **BACKSPACE**: Elimina el √∫ltimo car√°cter (sentence[:-1]).
- **BORRAR_TODO**: Elimina todo lo que se ha escrito.
- **SPACE**: A√±ade un espacio en blanco.

Gesti√≥n de **Disparador √önico**: La variable *action_just_triggered* impide que la acci√≥n se repita infinitamente si el usuario no mueve la mano. La acci√≥n ocurre una sola vez, y luego el sistema espera a que el gesto cambie o la mano se mueva (evento "Key Up").

### Robustez y Gesti√≥n de Errores (Fault Tolerance)
Un **software** nunca debe fallar inesperadamente; por ello, ha sido blindado contra fallos cr√≠ticos mediante el uso estrat√©gico de bloques try...except.

1. **Carga de Recursos Externos**:
Al inicio, el script intenta cargar los iconos **PNG** (mic_blue.png, etc.). Si los archivos *faltan* (error com√∫n al mover el proyecto a otro PC), el c√≥digo intercepta la excepci√≥n y activa la funci√≥n de respaldo *create_dummy_icon*, generando recursos gr√°ficos procedimentales sobre la marcha.

```python
except Exception as e:
    print(f"‚ö†Ô∏è Error cargando iconos: {e}. Usando fallback.")
    icon_blue = create_dummy_icon(...)
```

2. **Pipeline de Reconocimiento**:
Incluso durante el *ciclo principal*, el procesamiento de **MediaPipe** o la predicci√≥n del modelo podr√≠an generar errores imprevistos (ej. valores NaN o divisiones por cero en casos l√≠mite).  
 Todo el bloque l√≥gico est√° protegido:

```python
try:
    features = get_normalized_landmarks(hand_landmarks)
    # ... l√≥gica de predicci√≥n ...
except Exception as e:
    display_text = "Err"
    # El programa contin√∫a ejecut√°ndose en lugar de cerrarse
```

Esto garantiza que un solo **frame corrupto** no termine la ejecuci√≥n de la aplicaci√≥n.

## Stack Tecnol√≥gico Utilizado
El proyecto ha sido desarrollado √≠ntegramente en Python, aprovechando su extenso ecosistema de librer√≠as para Inteligencia Artificial y procesamiento en tiempo real.

### 1.‚Å† ‚Å†Visi√≥n Artificial y Procesamiento de Im√°genes

**MediaPipe** *(Google)*: El n√∫cleo del sistema. Utilizada para la detecci√≥n y tracking de las manos (Hands Solution). Proporciona los 21 puntos de referencia (landmarks) en 3D.

**OpenCV** *(cv2)*: Fundamental para la captura de video, el pre-procesamiento de frames y el renderizado de toda la interfaz gr√°fica (GUI) din√°mica (dibujo de l√≠neas, rect√°ngulos y texto en pantalla).

**Pillow** *(PIL)*: Utilizada para la gesti√≥n avanzada de im√°genes, espec√≠ficamente para cargar y superponer los iconos de la interfaz (micr√≥fono, logos) manteniendo la transparencia (canal Alfa) que OpenCV no gestiona nativamente con facilidad.

### 2.‚Å† ‚Å†Machine Learning y Datos

**Scikit-learn**: Librer√≠a utilizada para entrenar el modelo de clasificaci√≥n. Se us√≥ el algoritmo Random Forest Classifier por su equilibrio entre precisi√≥n y velocidad de ejecuci√≥n.

**NumPy**: Esencial para las operaciones matem√°ticas de alto rendimiento. Se usa para convertir los landmarks en arrays, calcular distancias euclidianas y normalizar coordenadas.

**Pickle**: M√≥dulo utilizado para la serializaci√≥n del modelo entrenado (model.p), permitiendo guardarlo y cargarlo instant√°neamente sin re-entrenar cada vez.

### 3.‚Å† ‚Å†Interacci√≥n y Sistema

**Pyttsx3**: Librer√≠a de s√≠ntesis de voz (Text-to-Speech) offline. Permite que el sistema "lea" en voz alta la frase construida por el usuario.

**Threading**: M√≥dulo de la librer√≠a est√°ndar de Python. Crucial para ejecutar la s√≠ntesis de voz en un hilo separado, evitando que la interfaz de video se congele mientras el ordenador habla.

**Math**: Usada para c√°lculos trigonom√©tricos (necesarios para dibujar el arco de carga circular).

---
## Fuentes
### 1. Literatura Cient√≠fica y Algor√≠tmica 
Para el desarrollo del sistema de seguimiento de manos, nos hemos basado en la investigaci√≥n original de Google:
Lugares, F. et al. (2020). MediaPipe Hands: On-device Real-time Hand Tracking. Este documento sirvi√≥ como base te√≥rica para comprender la topolog√≠a de los 21 puntos de referencia (landmarks) y c√≥mo se extrae la coordenada Z (profundidad relativa).

### 2. Documentaci√≥n T√©cnica Oficial 
La implementaci√≥n del c√≥digo se ha guiado por la documentaci√≥n oficial de las librer√≠as utilizadas, garantizando el uso de buenas pr√°cticas:

**Scikit-learn Documentation**: Espec√≠ficamente la secci√≥n sobre Ensemble Methods y Random Forest Classifier para la optimizaci√≥n de hiperpar√°metros.

**OpenCV Documentation**: Para las funciones de procesamiento de im√°genes y dibujo de primitivas gr√°ficas.

**Python Software Foundation**: Para la gesti√≥n de hilos (threading) y manejo de excepciones.

### 3. Origen de los Datos (Enfoque H√≠brido)
Para el entrenamiento del modelo, se ha optado por una estrategia mixta que combina el volumen de datos p√∫blicos con la especificidad de muestras generadas ad-hoc:

**Datasets P√∫blicos (Base de Conocimiento)**: Se han seleccionado e integrado dos conjuntos de datos obtenidos de la plataforma Kaggle, "Spanish Sign Language Alphabet Static" y "Lenguaje de Signos Espa√±ol",Estos proporcionaron una base s√≥lida inicial con gran variabilidad de usuarios y formas de manos.

**Generaci√≥n de Datos Propios (Data Augmentation)**: Para adaptar el modelo a las condiciones operativas reales, se ampli√≥ el dataset base utilizando nuestra herramienta personalizada collect_data.py.

### 4. Herramientas de Asistencia e Inteligencia Artificial Generativa 
Como soporte a la productividad y la internacionalizaci√≥n del proyecto, se han utilizado modelos de lenguaje extensos (LLMs) bajo supervisi√≥n humana:

**Gemini (Google) y ChatGPT (OpenAI)**: Se han empleado como herramientas auxiliares para:
- Traducci√≥n y localizaci√≥n: Adaptaci√≥n precisa de los textos t√©cnicos y comentarios del c√≥digo del italiano al espa√±ol.
- Enriquecimiento del Diccionario: Generaci√≥n de listas de palabras de alta frecuencia para expandir el vocabulario del   sistema de autocompletado.
- Debugging asistido: An√°lisis preliminar de trazas de error (stack traces) para agilizar la identificaci√≥n de fallos sint√°cticos menores.

---

## Propuestas de ampliaci√≥n
- Se podr√≠a a√±adir la posibilidad de **traducir gestos din√°micos** y no solo est√°ticos, incorporando tambi√©n la **segunda mano** para la detecci√≥n.
- Se podr√≠a a√±adir la posibilidad de **modificar el idioma** en el que se quiere hablar y, en consecuencia, cambiar autom√°ticamente el **diccionario** seg√∫n el idioma seleccionado.
- **Ampliar el diccionario** con muchas m√°s palabras.

## Conclusi√≥n
Este proyecto nos ha resultado muy √∫til para aprender nuevas formas de aplicar las tecnolog√≠as vistas en clase en un contexto real y significativo. En particular, la idea de integrar un diccionario con palabras sugeridas para facilitar la escritura ha sido una manera concreta de reflexionar sobre los sistemas de asistencia che utilizamos desde hace a√±os en nuestros dispositivos m√≥viles, entendiendo mejor la l√≥gica y el dise√±o que hay detr√°s de este tipo de interfaces.  
El desarrollo del proyecto ha sido tambi√©n una verdadera prueba para comprender hasta d√≥nde pod√≠amos llegar, especialmente teniendo en cuenta el tiempo disponible y la complejidad de integrar distintas √°reas como visi√≥n artificial, aprendizaje autom√°tico, interfaz de usuario y accesibilidad.   
Trabajar en equipo, con una persona que comparte la misma motivaci√≥n y ganas de aprender, ha sido fundamental para afrontar las dificultades y avanzar de forma constante.  
Hemos mostrado el v√≠deo de presentaci√≥n del proyecto a amigos y familiares en Italia, y la reacci√≥n ha sido muy positiva, lo que nos ha confirmado el valor pr√°ctico y humano de la idea desarrollada. Esperamos que el proyecto resulte igualmente interesante y significativo para ustedes. Muchas gracias.
