# Buenos d√≠as profesor, somos Mattia Rizza y Riccardo Belletti y este es nuestro proyecto final de Visi√≥n por Computador
<img src="img/TraductorRBMR.png" width="500" />

[Link Projecto y Video y Dataset](https://alumnosulpgc-my.sharepoint.com/:f:/g/personal/mattia_rizza101_alu_ulpgc_es/IgC-0DDnFtsgSYzWowwpVGCvAVrgFpJ8NdjnmPI-oWncQes?e=cIqijs)

El objetivo del proyecto ha sido aplicar de manera pr√°ctica los conceptos vistos en clase queriendo crear un verdadero traductor para la **lengua de signos**.  
El proyecto no nace como un trabajo de investigaci√≥n avanzada, sino como un ejercicio completo y realista. Partimos de una idea que podr√≠a parecer banal inicialmente, le hemos a√±adido nuestro *toque personal* y hemos conseguido sacar un buen programa que realmente se podr√≠a usar para ayudar a las personas con esta *discapacidad*.  

Al lanzar el programa se abrir√° el **v√≠deo** y el usuario ver√° que nos encontramos en modo traductor; en este modo el usuario podr√° practicar para **aprender la lengua de signos espa√±ola** y los otros s√≠mbolos que hemos a√±adido para implementar acciones.  
Una vez que el usuario decide que quiere escribir un mensaje, podr√° hacer el **gesto** ü§ü con las manos para entrar en el modo escritura; dentro de este modo el usuario podr√° **componer cualquier mensaje letra por letra** y, una vez terminado, podr√° poner el dedo sobre el icono del **micr√≥fono** para hacer que el ordenador **pronuncie** la frase compuesta.  

Para **acelerar la escritura** hemos a√±adido una funcionalidad que te permite ver en pantalla **palabras recomendadas** mientras est√°s escribiendo, y por lo tanto, si *por ejemplo* est√°s escribiendo "pro" aparecer√°n algunas palabras como "proyecto" y poniendo el dedo encima completar√° la palabra.  

Adem√°s de los gestos para escribir las letras, nosotros hemos a√±adido a nuestro dataset otros **6 gestos** para realizar las siguientes acciones:

- entrar y salir del modo escritura
- borrar toda la frase escrita en modo escritura
- borrar solo la √∫ltima letra escrita
- a√±adir un espacio en la frase
- a√±adir el signo de interrogaci√≥n de apertura
- a√±adir el signo de interrogaci√≥n de cierre
  
A continuaci√≥n es posible ver la leyenda con todos los gestos utilizables. 


<img src="img/Legenda.jpeg" width="400" />

--- 

## Idea general del proyecto

La idea base ha sido mezclar algunos datasets de im√°genes organizados en clases encontrados en **Kaggle** y luego integrarlos con im√°genes hechas por *nosotros*, en aquellas letras que nuestro programa ten√≠a *dificultades* para reconocer correctamente.

En particular:

- hemos recopilado im√°genes en bruto (raw data) que ya estaban organizadas en carpetas
- hemos escrito scripts de Python para automatizar parte del proceso
- hemos usado un notebook de Jupyter para explorar y verificar el dataset

## Diario y Metodolog√≠a de trabajo
En lo que respecta al **diario** de este *proyecto final*, muy a menudo hemos trabajado de manera presencial, ya fuera despu√©s de las clases en la *biblioteca de la universidad* o en otra *biblioteca* cercana a **Las Canteras**.  
Por la metodolog√≠s de trabajo hemos realizado casi todo el proyecto *juntos* y de *forma presencial*, para que ambos pudi√©ramos entender bien lo que hac√≠a el otro y porque ante cualquier **problema** o **duda**, en *persona* se consigue resolver casi de inmediato, en lugar de hacerlo por *tel√©fono*.    
Las pocas veces en las que no consegu√≠amos encontrarnos en persona, utiliz√°bamos **videollamadas por WhatsApp** o, cuando uno pod√≠a y el otro no, trabaj√°bamos de **forma individual** envi√°ndonos mensajes cada vez que se realizaba alguna modificaci√≥n o avance.

Progetto: Traduttore di Lingua dei Segni Spagnola (LSE) basato su Computer Vision
1. Setup dell‚ÄôAmbiente di Sviluppo
Per garantire riproducibilit√† e isolamento delle dipendenze, il progetto √® stato sviluppato all‚Äôinterno di un ambiente virtuale Anaconda.

conda create -n progetto_vc python=3.10
conda activate progetto_vc
pip install mediapipe==0.10.9
pip install pyttsx3
pip install pillow
La versione di Python 3.10 √® stata scelta per garantire piena compatibilit√† con MediaPipe e le librerie di supporto.

2. Dataset: Costruzione e Unione delle Fonti
Abbiamo utilizzato e unificato due dataset pubblici scaricati da Kaggle:
* Spanish Sign Language Alphabet Static
* Lenguaje de Signos Espa√±ol
L‚Äôobiettivo dell‚Äôunione √® stato aumentare la variet√† delle mani, delle angolazioni e delle condizioni di illuminazione, migliorando la capacit√† di generalizzazione del modello.
Durante lo sviluppo ci siamo accorti che alcune lettere (Y, X, W, V, T, H, F) risultavano poco affidabili. Per questo motivo:
* abbiamo raccolto manualmente nuove immagini tramite webcam (script collect_data.py);
* abbiamo sostituito progressivamente parte delle immagini dei dataset originali con dati raccolti da noi, pi√π coerenti con l‚Äôambiente reale di utilizzo.

3. Pre-processing e Standardizzazione dei Dati (utils.py)
Il file utils.py rappresenta il cuore matematico del progetto: funge da ‚Äútraduttore‚Äù tra la visione artificiale e il modello di Machine Learning.
3.1 Obiettivi del Pre-processing
Il pre-processing √® stato progettato per garantire:
* Invarianza alla traslazione‚Ä®Il gesto deve essere riconosciuto indipendentemente dalla posizione della mano nell‚Äôimmagine.
* Invarianza di scala‚Ä®Il gesto deve essere riconosciuto sia con la mano vicina che lontana dalla webcam.
* Compatibilit√† con modelli di Machine Learning‚Ä®I dati devono essere trasformati in un vettore numerico adatto a un classificatore.
3.2 Pipeline di Elaborazione
La funzione pre_process_landmark applica i seguenti passaggi:
1. Copia di sicurezza‚Ä®Viene creata una deepcopy dei landmark per evitare di modificare i dati usati per il rendering grafico.
2. Relativizzazione delle coordinate
    * Il landmark 0 (polso) viene fissato come origine (0,0).
    * Tutti gli altri punti vengono espressi come differenza rispetto al polso.
3. Flattening‚Ä®La lista di coppie (x, y) viene trasformata in un unico vettore monodimensionale.
4. Normalizzazione‚Ä®Tutti i valori vengono scalati nell‚Äôintervallo [-1, 1], migliorando la stabilit√† numerica e la convergenza del modello.
Output finale:‚Ä®Un vettore di numeri reali pronto per essere fornito al classificatore.

4. Estrazione delle Feature (create_dataset.ipynb)
Questo notebook ha il compito di trasformare le immagini grezze in dati numerici.
Pipeline:
1. Caricamento delle immagini organizzate per classe (A, B, C, ‚Ä¶).
2. Rilevamento dei 21 landmark della mano tramite MediaPipe Hands.
3. Applicazione del pre-processing definito in utils.py.
4. Salvataggio dei dati in formato numerico.
Risultato: un dataset strutturato e pronto per l‚Äôaddestramento.

5. Addestramento del Modello (train_classifier.ipynb)
5.1 Scelte di Progetto
√à stato utilizzato un Random Forest Classifier perch√©:
* √® robusto al rumore;
* non richiede feature engineering complesso;
* funziona bene con dataset di dimensioni medio-piccole.
5.2 Fasi di Training
* Suddivisione dei dati:
    * 80% Training Set
    * 20% Test Set
* Addestramento del modello
* Valutazione tramite accuracy score
Se l‚Äôaccuratezza supera il 95%, il modello viene esportato come file statico:

model.p
Questo file rappresenta il ‚Äúcervello‚Äù dell‚Äôapplicazione finale.

6. Applicazione in Tempo Reale (inference_classifier.py)
Questo √® il file esecutivo, quello che l‚Äôutente finale utilizza.
6.1 Funzionalit√† Principali
* Acquisizione video dalla webcam
* Rilevamento della mano
* Conversione dei dati visivi in dati matematici
* Predizione del segno
* Interfaccia grafica aumentata
6.2 Pipeline Logica
Fase A ‚Äì Setup
* Caricamento del modello model.p (se presente).
* Modalit√† fallback se il modello non √® disponibile.
Fase B ‚Äì Detection
* MediaPipe individua i 21 landmark.
* Disegno dello scheletro della mano a schermo.
Fase C ‚Äì Ponte Visione ‚Üí AI
* Conversione coordinate normalizzate ‚Üí pixel.
* Pre-processing tramite utils.py.
Fase D ‚Äì Inference
* Predizione numerica del modello.
* Traduzione numero ‚Üí lettera tramite dizionario.
Output visivo:
* Webcam live
* Bounding box della mano
* Lettera riconosciuta

7. Problema Critico: Distinzione tra T e F (Profondit√†)
Le lettere T e F risultano quasi indistinguibili in 2D (effetto ‚Äúombra cinese‚Äù).
7.1 Analisi del Problema
* In una webcam 2D le coordinate (x, y) sono quasi identiche.
* Aggiungere immagini al dataset portava a overfitting.
7.2 Soluzione Algoritmica
Abbiamo sfruttato la coordinata Z stimata da MediaPipe:
* Calcolo della differenza di profondit√† tra:
    * punta dell‚Äôindice
    * punta del pollice
Regola:
* indice pi√π vicino alla camera ‚Üí F
* indice allineato o dietro ‚Üí T
7.3 Calibrazione Sperimentale
* F: valori fino a -0.036
* T: valori ~ -0.024
* Soglia finale: -0.028
Risultato: distinzione stabile e riproducibile senza riaddestrare il modello.

8. Modalit√† Scrittura e Gestione dei Comandi
Abbiamo introdotto segni speciali per:
1. Entrare / uscire dalla modalit√† scrittura
2. Inserire spazi
3. Cancellare tutto
4. Cancellare ultimo carattere
5. Inserire il punto interrogativo
Questo trasforma il riconoscitore in un vero sistema di scrittura gestuale.

9. Text-to-Speech (Accessibilit√†)
Per rendere il sistema realmente utile a persone con difficolt√† vocali, abbiamo integrato la sintesi vocale.
* Libreria: pyttsx3 (offline)
* Voce: spagnola (ricerca automatica nel sistema)
* Attivazione: uscita dalla modalit√† scrittura
Quando l‚Äôutente termina la frase, il sistema legge ad alta voce il testo prodotto.

10. Supporto Unicode (√ë, ¬ø)
OpenCV non supporta correttamente caratteri Unicode.‚Ä®Abbiamo quindi integrato Pillow per il rendering del testo:
* Supporto completo a:
    * √ë
    * ¬ø
* Font reali (Arial)
* Testo pulito e leggibile

11. Suggeritore Predittivo (NLP Lite)
Abbiamo implementato un sistema di suggerimento lessicale:
* Dizionario interno con ~100 parole spagnole frequenti
* Analisi dell‚Äôultima parola in tempo reale
* Visualizzazione di suggerimenti dinamici
Interazione Touchless
I suggerimenti sono cliccabili senza mouse:
* Hover con l‚Äôindice
* Barra di caricamento temporale
* Selezione automatica

12. Interfaccia Grafica: Icona del Microfono
Abbiamo aggiunto un feedback visivo tramite icone PNG con trasparenza:
* mic_blue.png ‚Üí stato idle
* mic_yellow.png ‚Üí hover
* mic_green.png ‚Üí parlato
Se le icone non sono presenti, il sistema usa un fallback grafico, evitando crash.

13. Risultato Finale
Il progetto integra:
* Computer Vision
* Machine Learning
* Sintesi vocale
* NLP
* Interfaccia touchless
Non si tratta solo di ‚Äúusare una libreria‚Äù, ma di progettare un sistema completo, robusto e orientato all‚Äôaccessibilit√†.



## Estructura del proyecto

La estructura principal del repositorio es la siguiente:
```
Progetto_VC/
‚îÇ
‚îú‚îÄ‚îÄ pycache/
‚îÇ
‚îú‚îÄ‚îÄ inference_classifier.py
‚îÇ
‚îú‚îÄ‚îÄ model.p
‚îÇ
‚îú‚îÄ‚îÄ test_vision.py
‚îú‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ create_dataset.ipynb
‚îú‚îÄ‚îÄ train_classifier.ipynb
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ collect_data.py
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ raw/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ABRIR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_LETRA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ BORRAR_TODO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ CERRAR_INTERROGACION/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ESPACIO/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ F/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ H/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MODO_ESCRITURA/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ S/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ T/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ U/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ V/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ W/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ X/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ Y/
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ new_data/
‚îÇ
‚îÇ
‚îî‚îÄ‚îÄ .DS_Store
```


## Descripci√≥n de las carpetas y de los archivos principales

### utils.py

**Objetivo del m√≥dulo**
El archivo **utils.py** contiene la *l√≥gica matem√°tica de transformaci√≥n de los datos*. Su funci√≥n principal, *get_normalized_landmarks*, act√∫a como un filtro intermedio entre la extracci√≥n en bruto de *+MediaPipe** y la entrada del clasificador.   
El *objetivo* es hacer que los datos sean agn√≥sticos respecto a la posici√≥n y a la distancia de la mano, garantizando que el modelo aprenda la forma del gesto y no su posici√≥n en el espacio.

**Funcionamiento t√©cnico**  
La funci√≥n recibe como entrada el objeto **hand_landmarks** de MediaPipe y aplica una pipeline de transformaci√≥n en tres fases:

**1. Conversi√≥n a coordenadas relativas (invarianza a la traslaci√≥n)**  
Los datos en bruto de MediaPipe son coordenadas absolutas (x, y) normalizadas respecto a las dimensiones de la imagen (0.0 - 1.0). Si us√°ramos estos datos directamente, el modelo aprender√≠a que una mano en la esquina superior izquierda es diferente de una mano en la esquina inferior derecha, aunque hagan el mismo gesto. Para resolver este problema, el c√≥digo establece la mu√±eca (Landmark 0) como origen (0, 0) del sistema cartesiano local. Resta las coordenadas de la mu√±eca a todos los dem√°s puntos:

```python
P'{i} = P{i} - P_{polso}
```
Encuentre las coordenadas de la mu√±eca (punto 0) para utilizarlas como origen.
```python
if index == 0:
    base_x, base_y = landmark_point[0], landmark_point[1]
```
Resta la base a todos los puntos (traslaci√≥n del origen)
```python
temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x
temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y
```


**2. Flattening (aplanamiento)**  
Los datos se convierten de una lista de parejas bidimensionales [[x1, y1], [x2, y2]...] a un √∫nico vector unidimensional [x1, y1, x2, y2...].

Aplana la lista utilizando itertools.
```python
temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))
```

**3. Normalizaci√≥n de escala (invarianza a la escala)**
La **mano** puede estar cerca de la c√°mara (coordenadas grandes) o lejos (coordenadas peque√±as). Para hacer que el gesto sea reconocible independientemente de la distancia, los valores se normalizan dividiendo todo por el valor absoluto m√°ximo presente en el vector. Esto fuerza a que todos los datos queden dentro de un rango entre ‚àí 1 y 1.

Normalizza tra -1 e 1
```python
max_value = max(list(map(abs, temp_landmark_list)))

def normalize_(n):
    return n / max_value if max_value != 0 else 0

temp_landmark_list = list(map(normalize_, temp_landmark_list))
```

### create_database.ipynb

**Objetivo del notebook**  
Este script constituye la fase de **Pre-processing** y **Feature Extraction** de la pipeline de Computer Vision. El objetivo no es simplemente leer las im√°genes, sino transformar los datos no estructurados (p√≠xeles de las im√°genes raw) en datos estructurados (coordenadas geom√©tricas de los landmark de la mano), listos para el entrenamiento de un clasificador (por ejemplo Random Forest).

En concreto, el notebook realiza tres tareas cr√≠ticas:

1. **Iteraci√≥n**: Escanea el dataset organizado en directorios.  
2. **Feature Extraction**: Utiliza MediaPipe Hands para detectar el esqueleto de la mano en cada imagen y extraer las coordenadas (x, y) de los 21 puntos clave.  
3. **Serializaci√≥n**: Guarda las listas de features y las etiquetas (labels) en un formato binario comprimido (data.pickle), reduciendo dr√°sticamente el tama√±o de los datos respecto a las im√°genes originales y acelerando el training.

**Requisitos previos y librer√≠as**
Para la ejecuci√≥n correcta, la estructura de directorios debe seguir la taxonom√≠a de clases (por ejemplo data/A, data/B, etc.). Las librer√≠as principales son:

- MediaPipe: para la extracci√≥n de los landmark esquel√©ticos (el ‚Äúcoraz√≥n‚Äù del pre-processing).
- OpenCV (cv2): para la manipulaci√≥n de im√°genes (conversi√≥n BGR -> RGB).
- Pickle: para la serializaci√≥n de objetos Python.
- Matplotlib (opcional): para visualizar las im√°genes durante el debug.

**An√°lisis de la estructura (detalle a nivel de c√≥digo)**  
Celda 1 ‚Äì **Configuraci√≥n del entorno**
Se definen las rutas y se inicializa el modelo est√°tico de MediaPipe. A diferencia del script en tiempo real, aqu√≠ configuramos MediaPipe con static_image_mode=True, optimizado para im√°genes individuales con alta precisi√≥n.

```python
mp_hands = mp.solutions.hands

hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)
DATA_DIR = './data'
```

Celda 2 ‚Äì **Extracci√≥n de las features (Core Loop)**  
Esta es la secci√≥n computacionalmente m√°s intensa. El c√≥digo itera sobre cada subcarpeta (que representa una clase/letra) y para cada imagen ejecuta la conversi√≥n.

Pasos t√©cnicos relevantes para cada imagen:

1. **Conversi√≥n de espacio de color**: MediaPipe requiere entrada RGB, mientras que OpenCV carga en BGR.
```python
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
```

2. **Inferencia MediaPipe: se calculan los landmark.**

```python
results = hands.process(img_rgb)
```

3. **Feature Extraction & Normalizaci√≥n** (crucial):
si se detecta una mano, no nos limitamos a extraer coordenadas crudas (x, y respecto a los bordes de la imagen). En su lugar, se invoca la funci√≥n *custom get_normalized_landmarks*

```python
if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            normalized_landmarks = get_normalized_landmarks(hand_landmarks)
            data.append(normalized_landmarks)
            labels.append(dir_)
```

Celda 3 ‚Äì **Serializaci√≥n de los datos**
Los datos procesados se guardan. Este paso crea un ‚Äúcheckpoint‚Äù. Si en el futuro se quiere cambiar el modelo de clasificaci√≥n (por ejemplo pasar de Random Forest a SVM o Red Neuronal), no ser√° necesario reprocesar todas las im√°genes, sino que bastar√° con cargar este archivo pickle.

```python
f = open('data.pickle', 'wb')
pickle.dump({'data': data, 'labels': labels}, f)
f.close()
```

### train_classifier.ipynb

**Objetivo del notebook**  
En este script ocurre la **transici√≥n** desde los datos geom√©tricos (las coordenadas de los landmark extra√≠das en el paso anterior, create_database.ipynb) hasta la creaci√≥n de un modelo de decisi√≥n capaz de clasificar nuevas entradas en tiempo real.

El objetivo es entrenar un algoritmo de Aprendizaje Supervisado para que aprenda a asociar patrones espec√≠ficos de coordenadas (features) con las letras correspondientes (label).

**Librer√≠as utilizadas**  
- *Scikit-learn* (sklearn): librer√≠a est√°ndar de facto para ML en Python. Se utiliza para la gesti√≥n del dataset, creaci√≥n del modelo y c√°lculo de m√©tricas.
- *Pickle & NumPy*: para gesti√≥n eficiente de datos serializados y operaciones matriciales.

**An√°lisis del flujo (detalle t√©cnico)**  
Celdas 1 & 2 ‚Äì **Carga y preparaci√≥n de datos**
El notebook comienza cargando el archivo **dataset.pickle** generado en la fase anterior. Las listas Python se convierten inmediatamente en **NumPy Arrays**, optimizados para c√°lculos vectoriales requeridos por los algoritmos de Scikit-learn, ofreciendo prestaciones superiores respecto a listas est√°ndar.

Celda 3 ‚Äì **Data Splitting y entrenamiento** (el core)
Esta celda ejecuta tres operaciones cr√≠ticas para la validez cient√≠fica del proyecto:

1. **Partitioning** (Train/Test Split): el dataset se divide en dos subconjuntos disjuntos:

**Training Set** *(80%)*: usado por el modelo para aprender las reglas.
**Test Set** *(20%)*: usado para evaluar el rendimiento en datos ‚Äúnunca vistos antes‚Äù.

```python
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)
```

2. **Selecci√≥n del modelo:** se eligi√≥ *Random Forest Classifier*.  
*Motivaci√≥n:* es un m√©todo *‚ÄúEnsemble‚Äù* que construye una multitud de √°rboles de decisi√≥n. Es especialmente adecuado para este proyecto porque gestiona bien datasets con muchas features *(42 coordenadas en total)* y es robusto frente al overfitting (el riesgo de aprender ‚Äúde memoria‚Äù en lugar de generalizar).

3. **Evaluaci√≥n (Accuracy)**: despu√©s del entrenamiento (.fit), el modelo genera predicciones sobre el Test Set. La exactitud (accuracy_score) nos proporciona una m√©trica porcentual fiable sobre la capacidad del modelo para generalizar.

```python
model = RandomForestClassifier()
model.fit(x_train, y_train)
# Haz una prueba con los datos de test para ver qu√© tan bueno es
y_predict = model.predict(x_test)
# Calcula la accuracy
score = accuracy_score(y_predict, y_test)
```
Exactitud del modelo: 99.26%.

Celda 4 ‚Äì **Serializaci√≥n del modelo**
Una vez verificada una exactitud satisfactoria *(t√≠picamente > 95%)*, el **modelo entrenado** se guarda en el archivo **model.p**.  
Este archivo contiene el objeto completo **Random Forest** (con todos sus √°rboles de decisi√≥n y los umbrales matem√°ticos calculados) y ser√° el √∫nico archivo necesario para el script de inferencia en tiempo real (inference_classifier.py).

```python
f = open('model.p', 'wb')
pickle.dump({'model': model}, f)
f.close()
```

### collect_data.py

**Motivaci√≥n y necesidad del script**

Durante las fases preliminares del proyecto, se intent√≥ el entrenamiento utilizando exclusivamente la fusi√≥n de dos datasets p√∫blicos preexistentes. Sin embargo, las pruebas iniciales evidenciaron dos criticidades fundamentales:

1. **Heterogeneidad de los datos**: los datasets originales presentaban condiciones de iluminaci√≥n, fondos y √°ngulos demasiado diferentes respecto al entorno operativo real, llevando a una baja capacidad de generalizaci√≥n del modelo (Domain Shift).

2. **Incompletitud de las clases:** no fue posible encontrar un dataset externo que cubriera perfectamente todas las clases deseadas.

Para *resolver estas problem√°ticas* sin tener que anotar manualmente miles de im√°genes, se desarroll√≥ el script **collect_data.py**. Esta herramienta permite integrar el dataset existente con im√°genes adquiridas directamente en el entorno de uso final, mejorando dr√°sticamente la robustez del modelo.

**Funcionamiento t√©cnico**
El script implementa un sistema de **adquisici√≥n on-demand**. A diferencia de una grabaci√≥n de v√≠deo continua, este enfoque permite al usuario posicionar la mano correctamente y guardar el *frame* solo cuando el gesto es perfecto, garantizando calidad del dato de entrada.  
El funcionamiento se basa en tres bloques l√≥gicos:

1. **Setup de la c√°mara** (alta resoluci√≥n)
Se inicializa la webcam con una resoluci√≥n **HD (1280x720)**.  
Usar una resoluci√≥n m√°s alta en esta fase es crucial para garantizar que MediaPipe (en el siguiente paso) reciba suficientes detalles para extraer los landmark con precisi√≥n.

```python
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
```

2. **Gesti√≥n din√°mica de clases** (File System)
El c√≥digo no requiere pre-crear las carpetas manualmente. Usando la librer√≠a os, el script verifica la entrada del teclado y gestiona autom√°ticamente la estructura de directorios. Si el usuario pulsa la tecla "A", el script comprueba la existencia de la carpeta ./data/raw/A, la crea si es necesario, y calcula el nombre progresivo del archivo para evitar sobrescrituras.

```python
# Convertimos el c√≥digo de la tecla en letra (ej. 97 -> 'a' -> 'A')
lettera = chr(key).upper()
 
# Gesti√≥n autom√°tica de la estructura de las carpetas
folder_path = os.path.join(DATA_DIR, lettera)
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
```

3. **Adquisici√≥n y guardado (I/O)**
En el momento en que se pulsa la tecla, el frame actual se *‚Äúcongela‚Äù* y se guarda en disco mediante **OpenCV**. Esto permite poblar r√°pidamente las clases menos representadas o a√±adir nuevas (como los comandos gestuales personalizados) en pocos segundos.

```python
# Cuenta cu√°ntos archivos ya existen para no sobrescribirlos
count = len(os.listdir(folder_path))
          
# Guarda la imagen
file_name = f"aa{count}.jpg"
cv2.imwrite(os.path.join(folder_path, file_name), frame)
```

### inference_classifier.py

**La infraestructura software y los motores de soporte**

El script **inference_classifier.py** no act√∫a como un simple ejecutor lineal, sino que se configura como un hub de integraci√≥n que orquesta simult√°neamente visi√≥n artificial, interfaces gr√°ficas avanzadas, s√≠ntesis vocal y l√≥gica predictiva.

Para superar los **l√≠mites nativos** de las librer√≠as individuales (como la falta de soporte de transparencia en OpenCV o las operaciones bloqueantes del audio), fue necesario implementar una capa de infraestructura custom antes de entrar en el ciclo principal de procesamiento.

**El motor gr√°fico avanzado** *(Alpha Blending)*

Uno de los retos al desarrollar interfaces modernas con **OpenCV** es la gesti√≥n de la *transparencia*. **OpenCV** gestiona las im√°genes como matrices de p√≠xeles *BGR (Blue-Green-Red)* opacos. Para visualizar iconos modernos *(como el micr√≥fono)* con bordes suaves y fondos transparentes, se implement√≥ la funci√≥n overlay_transparent.

Esta funci√≥n ejecuta una operaci√≥n matem√°tica conocida como Alpha Blending. En lugar de sobrescribir directamente los p√≠xeles del v√≠deo con los del icono (lo que resultar√≠a en un antiest√©tico rect√°ngulo negro alrededor de la imagen), el c√≥digo calcula una media ponderada para cada p√≠xel bas√°ndose en su nivel de transparencia.

Analizando el c√≥digo, vemos primero la separaci√≥n y normalizaci√≥n de los canales:

```python
# Separa los canales: BGR (color) y Alpha (transparencia)
overlay_img = overlay_resized[:, :, :3] 
overlay_mask = overlay_resized[:, :, 3:] / 255.0
```

Posteriormente, se calcula la m√°scara inversa para el fondo (donde el icono es transparente, el fondo debe verse):
```python
background_mask = 1.0 - overlay_mask
```

Finalmente, ocurre la fusi√≥n matricial mediante √°lgebra lineal con NumPy:
```python
# Fusiona las im√°genes: (Color del icono * Alpha) + (Fondo * (1 - Alpha))
blended_roi = (overlay_img * overlay_mask + roi * background_mask).astype(np.uint8)
```

Esta √∫nica l√≠nea de c√≥digo vectorial permite obtener una interfaz de usuario fluida.

**Renderizado de texto Unicode** (El puente *OpenCV-Pillow*)  

Otra *limitaci√≥n cr√≠tica* de **OpenCV** es la falta de soporte para **conjuntos de caracteres extendidos** (Unicode). Funciones est√°ndar como *cv2.putText* no son capaces de renderizar caracteres como la **√ë** espa√±ola o el signo de interrogaci√≥n invertido **¬ø**.  
Para resolver el problema, se cre√≥ la funci√≥n **wrapper put_text_utf8**.  
Esta funci√≥n act√∫a como un puente entre dos librer√≠as gr√°ficas distintas:

1. Convierte el frame de v√≠deo del formato OpenCV (array NumPy) al formato Pillow (PIL Image):

```python
img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
```

2. Utiliza el motor de renderizado de Pillow para dibujar el texto usando una fuente TrueType (arial.ttf), que soporta nativamente todos los glifos internacionales.

3. Reconvierten la imagen procesada al formato BGR de OpenCV para poder mostrarla en v√≠deo.

Este enfoque h√≠brido garantiza que la interfaz de usuario sea ling√º√≠sticamente correcta sin sacrificar el rendimiento de la pipeline de v√≠deo.
Adem√°s de la conversi√≥n de formatos, la funci√≥n implementa dos mecanismos de seguridad importantes:

1) Fallback de Tipograf√≠a (Portabilidad): El sistema intenta cargar la fuente vectorial arial.ttf para asegurar una est√©tica moderna. Sin embargo, dado que las fuentes disponibles var√≠an seg√∫n el sistema operativo, se encapsul√≥ la carga en un bloque de manejo de errores. Si el archivo no se encuentra, el sistema carga autom√°ticamente una fuente predeterminada en lugar de detener la ejecuci√≥n:
```python
try:
    font = ImageFont.truetype("arial.ttf", font_size)
except IOError:
    # Mecanismo de seguridad si falta la fuente
    font = ImageFont.load_default()
```
2) Correcci√≥n del Espacio de Color: Existe una discrepancia entre c√≥mo las librer√≠as interpretan los colores: OpenCV utiliza el est√°ndar BGR (Blue-Green-Red), mientras que Pillow utiliza RGB. Si pas√°ramos el color directamente, el texto rojo aparecer√≠a azul y viceversa. Por ello, el c√≥digo realiza una inversi√≥n manual de los canales de color antes de dibujar:
```python
# Inversi√≥n de canales: de BGR (OpenCV) a RGB (Pillow)
color_rgb = (color[2], color[1], color[0])
```

**Gesti√≥n As√≠ncrona del Audio y Arquitectura Multihilo** (*Multithreading*)  

Uno de los desaf√≠os cr√≠ticos en los sistemas interactivos en tiempo real es la gesti√≥n de la latencia. La operaci√≥n m√°s costosa en t√©rminos de tiempo de ejecuci√≥n no es el reconocimiento de imagen, sino la s√≠ntesis vocal.

La librer√≠a pyttsx3 opera nativamente en modo bloqueante: la funci√≥n engine.runAndWait() detiene la ejecuci√≥n del procesador hasta que la frase completa ha sido pronunciada. Si el ordenador debe decir "Hola, ¬øc√≥mo est√°s?", el proceso tarda entre 2 y 3 segundos. En una arquitectura de un solo hilo (Single-Threaded), esto implicar√≠a congelar el flujo de v√≠deo de la webcam durante ese tiempo, destruyendo la experiencia de usuario.

Para resolver este cuello de botella y mantener el sistema fluido a 30 FPS, se implement√≥ una arquitectura Multihilo (Multithreading) que desacopla el bucle de renderizado (V√≠deo) del bucle de procesamiento (Audio).
1. Orquestaci√≥n de Hilos (run_voice_thread)
La funci√≥n run_voice_thread act√∫a como el punto de entrada para la ejecuci√≥n concurrente. En lugar de ejecutar el audio directamente, instancia un Worker Thread:
```python
def run_voice_thread(text):
    t = threading.Thread(target=speak_function, args=(text, VOICE_ID_MANUALE))
    t.start()
```
Desacoplamiento: Al invocar t.start(), el sistema operativo crea un nuevo flujo de ejecuci√≥n paralelo.

Resultado: El Main Thread (encargado del v√≠deo y la IA) queda libre inmediatamente para procesar el siguiente frame, mientras que el audio se procesa en segundo plano.

2. L√≥gica de Configuraci√≥n Din√°mica (speak_function)
La funci√≥n speak_function, que se ejecuta dentro del hilo secundario, no se limita a reproducir sonido. Implementa una l√≥gica robusta de autconfiguraci√≥n y localizaci√≥n para garantizar que el sistema funcione correctamente en diferentes ordenadores.

Analizando el c√≥digo, vemos tres pasos clave:

A. Selecci√≥n Autom√°tica del Idioma: Dado que el proyecto est√° dise√±ado para la Lengua de Signos Espa√±ola, el sistema no asume una configuraci√≥n predeterminada. En su lugar, itera sobre los drivers de voz instalados en el sistema operativo buscando expl√≠citamente una voz hispana:
```python
voices = engine.getProperty('voices')
for v in voices:
    # B√∫squeda heur√≠stica de drivers en espa√±ol
    if "spanish" in v.name.lower() or "esp" in v.name.lower():
        engine.setProperty('voice', v.id)
        break
```
Este algoritmo de b√∫squeda garantiza la portabilidad del software: funcionar√° tanto en un Windows configurado en ingl√©s como en uno en espa√±ol, siempre que exista un paquete de voz compatible.

B. Configuraci√≥n de Velocidad: Se ajusta la velocidad de habla (rate) a 140 palabras por minuto para asegurar una dicci√≥n clara y natural, adecuada para fines educativos o de asistencia.
```python
engine.setProperty('rate', 140)
```
C. Tolerancia a Fallos: Toda la l√≥gica de audio est√° encapsulada en un bloque try...except.
```python
except Exception as e: print(f"Errore Audio: {e}")
```

**El motor NLP (Natural Language Processing)**

Por √∫ltimo, para elevar la experiencia de usuario con la funcionalidad de ‚ÄúSugeridor Inteligente‚Äù, se implement√≥ un motor de NLP ligero y determinista.

La decisi√≥n t√©cnica de no utilizar redes neuronales profundas (como LSTM, Transformers o BERT) para esta tarea espec√≠fica fue dictada por la necesidad de priorizar la baja latencia. En un sistema de visi√≥n artificial que ya consume recursos de GPU/CPU para procesar 30 im√°genes por segundo, a√±adir un modelo de lenguaje pesado habr√≠a comprometido la fluidez del v√≠deo.

Estructura y Algoritmo: El sistema se apoya en una Knowledge Base est√°tica (la lista DICCIONARIO), que ha sido curada manualmente para incluir:
- Palabras de uso com√∫n (HOLA, GRACIAS, POR FAVOR).
- Vocabulario espec√≠fico del contexto acad√©mico/universitario (PROYECTO, PROFESOR, EXAMEN, VISI√ìN).

La funci√≥n get_suggestions_list implementa un algoritmo de B√∫squeda de Prefijos (Prefix Matching). Analiza la frase en construcci√≥n en tiempo real y a√≠sla el √∫ltimo fragmento escrito para ofrecer candidatos compatibles.

```python
def get_suggestions_list(current_sentence):
    if not current_sentence: return []
    parts = current_sentence.split(" ")
    last_fragment = parts[-1] # A√≠sla el sufijo actual (ej. "PR")
    if len(last_fragment) == 0: return [] 
    matches = []
    for word in DICCIONARIO:
        # Busca palabras que empiecen por el fragmento (startswith)
        # y evita sugerir la palabra si ya est√° completa
        if word.startswith(last_fragment) and word != last_fragment:
            matches.append(word)
            # Optimizaci√≥n: Early Exit al encontrar 3 candidatos para no saturar la UI
            if len(matches) >= 3: break 
    return matches
```

Este dise√±o permite obtener sugerencias instant√°neas (complejidad computacional m√≠nima) que se actualizan frame a frame mientras el usuario compone el gesto.

**Arquitectura del Ciclo de Ejecuci√≥n (Runtime Loop)**
Una vez inicializados los subsistemas de soporte (Gr√°ficos, Audio, NLP), el control del programa pasa al n√∫cleo operativo.

El script est√° encapsulado en un bucle infinito (while True), que act√∫a como orquestador central gestionando la sincronizaci√≥n estricta entre la adquisici√≥n del mundo real (Webcam) y el renderizado de la informaci√≥n digital (GUI).

**Adquisici√≥n y normalizaci√≥n del flujo de v√≠deo** 
Al inicio de cada iteraci√≥n, el sistema adquiere el frame bruto de la c√°mara.
```python
while True:
    ret, frame = cap.read()
    if not ret: break
```
Sin embargo, antes de pasar a la fase de inferencia o dibujo, se ejecutan dos operaciones cr√≠ticas de pre-processing para adecuar los datos:

- Conversi√≥n de espacio de color: MediaPipe, al estar entrenado sobre datasets RGB, requiere este formato espec√≠fico, mientras   que OpenCV adquiere nativamente en BGR. La conversi√≥n es necesaria para garantizar la precisi√≥n del modelo.
- Mirroring (Efecto espejo): Esta operaci√≥n es fundamental para la Usabilidad (UX). Sin el volteo horizontal (flip), mover la   mano f√≠sica hacia la derecha provocar√≠a un movimiento hacia la izquierda en la pantalla (como una c√°mara de vigilancia),     creando una disonancia cognitiva que har√≠a imposible interactuar con los botones.

#### Rendering dell'Interfaccia Dinamica (GUI)
L'interfaccia utente non √® statica, ma contestuale: cambia in base allo stato del sistema. Il codice utilizza una logica condizionale per decidere cosa disegnare.
Modalit√† Scrittura vs. Attesa: Il booleano is_writing_mode funge da gatekeeper grafico:
- Se False (Attesa): L'interfaccia √® minimalista (barra grigia), invitando l'utente a fare il gesto di attivazione ("ROCK").
- Se True (Scrittura): Viene renderizzata la "Dashboard" completa:
  - La Barra Verde in alto, che ospita la frase in costruzione .
  - Il Pulsante Microfono, che non √® un'immagine fissa ma un oggetto a stati (Blu = Riposo, Giallo = Hover, Verde = Attivo).
  - I Box dei Suggerimenti, generati dinamicamente iterando sulla lista current_suggestions.

#### Logica dei Pulsanti Virtuali (Touchless Interaction)Uno degli aspetti pi√π innovativi del progetto √® l'implementazione di pulsanti cliccabili senza contatto fisico. Poich√© non esiste un mouse o un touch screen, il sistema deve simulare il "click" usando solo la posizione della mano.Questo viene realizzato attraverso un algoritmo in tre fasi: Mapping, Collision Detection e Temporal Filtering.A. Mapping delle CoordinateMediaPipe restituisce coordinate normalizzate ($0.0 \rightarrow 1.0$). Per interagire con la GUI, queste devono essere proiettate nello spazio pixel dello schermo ($1280 \times 720$):
```python
index_x = int((1 - hand_landmarks.landmark[8].x) * W)
index_y = int(hand_landmarks.landmark[8].y * H)
```

B. Collision Detection (Rilevamento Collisioni)
Il sistema verifica se il punto $(x, y)$ dell'indice cade all'interno del rettangolo di un pulsante (Bounding Box). Esempio per il tasto "PARLA":
```python
if BTN_PARLA_X < index_x < (BTN_PARLA_X + BTN_PARLA_W) and \
   BTN_PARLA_Y < index_y < (BTN_PARLA_Y + BTN_PARLA_H):
       is_hovering_any_ui = True
```
C. Temporal Filtering (Dwell Time) Il problema principale delle interfacce gestuali √® l'effetto "Midas Touch": si rischia di cliccare tutto ci√≤ che si tocca per sbaglio. Per evitare falsi positivi, √® stato implementato un meccanismo di Dwell Time (tempo di permanenza). L'utente deve mantenere il dito sul pulsante per un tempo prefissato (es. 1.0 secondo) per confermare l'intenzione.
```python
elapsed = time.time() - hover_start_time
```
E fornisce un Feedback Visivo Progressivo (Barra di caricamento o cambio colore):
```python
# Disegna barra di caricamento gialla proporzionale al tempo trascorso
load_w = int((elapsed / 1.0) * BTN_W)
cv2.rectangle(frame, ..., (BTN_X + load_w, ...), (0, 255, 255), -1)
```
Solo quando elapsed >= 1.0, l'evento viene scatenato (action_triggered_flag = True) e il comando viene eseguito (es. avvio del thread vocale).4. Gestione Dinamica dei SuggerimentiI pulsanti dei suggerimenti non sono fissi. Ad ogni frame, se l'utente sta scrivendo, il sistema ricalcola le coordinate per $N$ pulsanti (dove $N$ √® la lunghezza di current_suggestions).
```python
for i, word in enumerate(current_suggestions):
    bx = SUGG_START_X + (SUGG_W + SUGG_GAP) * i
    # ... disegno rettangolo e testo ...
    # ... controllo collisione per ogni i-esimo pulsante ...
```
Questo design permette all'interfaccia di adattarsi: se non ci sono suggerimenti, i pulsanti spariscono; se ce ne sono 3, appaiono ordinatamente affiancati.
Una volta che il sistema ha rilevato che l'utente non sta interagendo con i pulsanti (quindi is_hovering_any_ui == False), entra in gioco la pipeline di riconoscimento gestuale.

Questa fase non si limita a chiedere "che lettera √®?", ma applica una serie di filtri logici e temporali per correggere gli errori tipici della visione artificiale.
Il primo passo √® l'interrogazione del modello Random Forest. Invece di chiedere semplicemente la classe vincente (model.predict), il codice richiede le probabilit√† (model.predict_proba).
```python
features = get_normalized_landmarks(hand_landmarks)
prediction_proba = model.predict_proba([np.asarray(features)])
max_prob = np.max(prediction_proba)
```
Questo permette di implementare un Filtro di Confidenza:
```python
if max_prob < MIN_CONFIDENCE:
    # Ignora il gesto se il modello non √® abbastanza sicuro
```
Questo impedisce al sistema di scrivere caratteri casuali quando la mano √® in transizione o in una posizione ambigua, riducendo drasticamente il "rumore" di fondo.

#### Correzione problemi
I modelli basati solo su immagini 2D spesso confondono gesti simili. Per risolvere questo problema, nel codice sono stati iniettati dei correttori logici basati sulla geometria 3D e sul tempo.

A. Correzione Geometrica 3D (T vs F) Le lettere 'T' e 'F' nella lingua dei segni sono molto simili, ma differiscono nella profondit√† (quale dito sta davanti). MediaPipe fornisce la coordinata Z (profondit√†). Il codice calcola la distanza relativa sull'asse Z tra la punta del pollice e quella dell'indice:
```python
diff_z = index_tip_z - thumb_tip_z
if diff_z < SOGLIA_LIMIT: predicted_character = 'F'
else: predicted_character = 'T'
```
B. Analisi Temporale Dinamica (N vs √ë) La 'N' e la '√ë' hanno la stessa forma della mano, ma la '√ë' prevede un movimento ondulatorio laterale. Un classificatore statico non pu√≤ vedere il movimento. Per risolvere ci√≤, il sistema mantiene una memoria storica (x_history) delle ultime 20 posizioni del polso.
```python
x_history.append(wrist_x)
if len(x_history) > 20: x_history.pop(0)

# Calcola l'ampiezza del movimento recente
movement = max(x_history) - min(x_history)
if predicted_character == 'N' and movement > SOGLIA_MOVIMENTO_N:
    predicted_character = '√ë'
```
Se il sistema rileva la forma "N" MA c'√® oscillazione significativa, "promuove" la predizione a "√ë".
#### Stabilizzazione Temporale (Anti-Flickering)
Una volta determinata la lettera (es. "A"), non possiamo scriverla subito. I modelli ML "sfarfallano" (es. A-A-B-A-A) centinaia di volte al secondo. Per evitare di scrivere "AAAAA", √® stato implementato un Timer di Conferma (CONFIRMATION_TIME = 1.5 secondi).
Il sistema verifica la stabilit√†:
```python
is_stable = (predicted_character == last_char_detected)
```
‚Ä¢	Se la lettera cambia, il timer si resetta.
‚Ä¢	Se la lettera rimane la stessa, il timer avanza.
Durante l'attesa, l'utente riceve un feedback visivo immediato: un cerchio di caricamento disegnato attorno alla mano (cv2.ellipse), che si riempie progressivamente. Questo comunica all'utente: "Ho capito che vuoi fare la A, tienila ferma ancora un attimo...".

#### La Macchina a Stati (Esecuzione Comandi)
Quando il timer scade (elapsed >= CONFIRMATION_TIME), il sistema esegue l'azione associata al gesto riconosciuto. Qui il codice agisce come una macchina a stati finiti.
‚Ä¢	Stato 1: Cambio Modalit√† (SWITCH) Se il gesto √® "MODO_ESCRITURA" (Rock), inverte lo stato booleano is_writing_mode.
```python
is_writing_mode = not is_writing_mode
```
‚Ä¢  Stato 2: Editing del Testo Se siamo in modalit√† scrittura, il gesto viene tradotto in manipolazione della stringa sentence:
‚Ä¢	Caratteri standard: Vengono appesi alla stringa.
‚Ä¢	BACKSPACE: Rimuove l'ultimo carattere (sentence[:-1]).
‚Ä¢	BORRAR_TODO: Rimuove tutto ci√≤ che √® stato scritto.
‚Ä¢	SPACE: Aggiunge uno spazio.
Gestione Trigger Unico: La variabile action_just_triggered impedisce che l'azione venga ripetuta all'infinito se l'utente non muove la mano. L'azione avviene una volta sola, poi il sistema attende che il gesto cambi o che la mano si sposti ("Key Up event").
Questa sezione finale analizza come il codice garantisce fluidit√† e stabilit√† operativa.
#### Gestione della Concorrenza (Il Problema del TTS)


NE ABBIAMO Gi√† PARLATO!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  MA FORSE NE PARLIAMO  MEGLIO ORA
L'operazione pi√π "costosa" in termini di tempo non √® il riconoscimento dell'immagine, ma la sintesi vocale. La funzione engine.runAndWait() della libreria pyttsx3 √® intrinsecamente bloccante: se il computer deve dire "Buongiorno, come stai?", impiega circa 2-3 secondi. In un'architettura a thread singolo (Single-Threaded), questo significherebbe congelare la webcam per 3 secondi.
Per risolvere questo collo di bottiglia, √® stato implementato il Multithreading.
Analizziamo la funzione run_voice_thread:
```python
def run_voice_thread(text):
    t = threading.Thread(target=speak_function, args=(text, VOICE_ID_MANUALE))
    t.start()
```
-  Disaccoppiamento: Quando l'utente preme il pulsante "PARLA", il sistema non esegue l'audio direttamente. Invece, istanzia un oggetto Thread.
 - Esecuzione Parallela: Il metodo .start() ordina al sistema operativo di creare un nuovo flusso di esecuzione (worker thread).
-  Risultato: Il ciclo while True principale (Main Thread) continua immediatamente a processare il frame successivo della webcam senza attendere. L'audio viene riprodotto in parallelo. Questo design pattern √® fondamentale nei sistemi Real-Time Interactive, separando il Rendering Loop (video) dal Processing Loop (audio).
#### Robustezza e Gestione degli Errori (Fault Tolerance)
Un software non deve mai, ed √® stato blindato contro i fallimenti critici attraverso l'uso strategico dei blocchi try...except.
‚Ä¢	Caricamento Risorse Esterne: All'avvio, lo script tenta di caricare le icone PNG (mic_blue.png, ecc.). Se i file mancano (errore comune quando si sposta il progetto su un altro PC), il codice intercetta l'eccezione e attiva la funzione di fallback create_dummy_icon, generando risorse grafiche procedurali al volo.
```python
except Exception as e:
    print(f"‚ö†Ô∏è Errore caricamento icone: {e}. Uso fallback.")
    icon_blue = create_dummy_icon(...)
```
‚Ä¢	Pipeline di Riconoscimento: Anche durante il ciclo principale, l'elaborazione di MediaPipe o la predizione del modello potrebbero generare errori imprevisti (es. valori NaN, divisioni per zero in casi limite). L'intero blocco logico √® protetto:
```python
try:
    features = get_normalized_landmarks(hand_landmarks)
    # ... logica di predizione ...
except Exception as e:
    display_text = "Err"
    # Il programma continua a girare invece di chiudersi
```
Questo garantisce che un singolo frame corrotto non termini l'applicazione.







## Tecnologie utilizzate

* **Python 3**
* **Jupyter Notebook**
* Librerie standard per la gestione di immagini e file (ad esempio `os`, `opencv`, `numpy`, quando necessario)

Non abbiamo utilizzato framework particolarmente avanzati perch√© l‚Äôobiettivo del progetto era soprattutto capire **il flusso di lavoro**, non ottimizzare le prestazioni.

---

## Propuestas de ampliaci√≥n
- Se podr√≠a a√±adir la posibilidad de **traducir gestos din√°micos** y no solo est√°ticos, incorporando tambi√©n la **segunda mano** para la detecci√≥n.
- Se podr√≠a a√±adir la posibilidad de **modificar el idioma** en el que se quiere hablar y, en consecuencia, cambiar autom√°ticamente el **diccionario** seg√∫n el idioma seleccionado.
- **Ampliar el diccionario** con muchas m√°s palabras.




